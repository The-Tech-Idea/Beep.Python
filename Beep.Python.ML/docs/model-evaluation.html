<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Evaluation - Beep.Python.ML</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <link href="assets/styles.css" rel="stylesheet">
</head>
<body>
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">
        <i class="bi bi-list"></i>
    </button>

    <!-- Theme Toggle -->
    <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
        <i class="bi bi-sun-fill" id="theme-icon"></i>
    </button>

    <div class="container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <!-- Navigation will be loaded dynamically -->
        </aside>

        <!-- Main Content -->
        <main class="content">
            <div class="content-wrapper">
                <!-- Page Header -->
                <div class="page-header">
                    <div class="header-content">
                        <h1><i class="bi bi-graph-up text-success"></i> Model Evaluation</h1>
                        <p class="lead">Comprehensive model evaluation methods, metrics, and performance analysis for machine learning models supporting .NET 6, 7, 8, and 9</p>
                        
                        <div class="achievement-badges">
                            <span class="badge bg-success"><i class="bi bi-check-circle"></i> Multiple Metrics</span>
                            <span class="badge bg-info"><i class="bi bi-graph-up"></i> Visual Reports</span>
                            <span class="badge bg-warning"><i class="bi bi-tools"></i> Cross-Validation</span>
                            <span class="badge bg-primary"><i class="bi bi-star"></i> Advanced Analysis</span>
                        </div>
                    </div>
                </div>

                <!-- Evaluation Overview -->
                <section class="section" id="evaluation-overview">
                    <h2>?? Model Evaluation Overview</h2>
                    <p>Beep.Python.ML provides comprehensive model evaluation capabilities with multiple metrics, visualization tools, and advanced analysis methods for both classification and regression tasks.</p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="feature-card">
                                <h4><i class="bi bi-bar-chart text-primary"></i> Classification Metrics</h4>
                                <ul>
                                    <li><strong>Accuracy:</strong> Overall correctness</li>
                                    <li><strong>Precision:</strong> True positive rate</li>
                                    <li><strong>Recall:</strong> Sensitivity measure</li>
                                    <li><strong>F1-Score:</strong> Harmonic mean of precision/recall</li>
                                    <li><strong>ROC AUC:</strong> Area under ROC curve</li>
                                    <li><strong>Confusion Matrix:</strong> Detailed error analysis</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="col-md-6">
                            <div class="feature-card">
                                <h4><i class="bi bi-graph-down text-success"></i> Regression Metrics</h4>
                                <ul>
                                    <li><strong>R Score:</strong> Coefficient of determination</li>
                                    <li><strong>MSE:</strong> Mean Squared Error</li>
                                    <li><strong>RMSE:</strong> Root Mean Squared Error</li>
                                    <li><strong>MAE:</strong> Mean Absolute Error</li>
                                    <li><strong>MAPE:</strong> Mean Absolute Percentage Error</li>
                                    <li><strong>Residual Analysis:</strong> Error distribution</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Classification Evaluation -->
                <section class="section" id="classification-evaluation">
                    <h2>?? Classification Model Evaluation</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-target"></i> Basic Classification Metrics</h3>
                        
                        <div class="code-example">
                            <h4>Get Classification Scores</h4>
                            <pre><code class="language-csharp">// Get basic classification metrics
var scores = mlManager.GetModelClassificationScore("customer_churn_model");
double accuracy = scores.Item1;
double f1Score = scores.Item2;

Console.WriteLine($"Model Performance:");
Console.WriteLine($"Accuracy: {accuracy:P2}");
Console.WriteLine($"F1-Score: {f1Score:F4}");

// Example output:
// Model Performance:
// Accuracy: 89.45%
// F1-Score: 0.8721</code></pre>
                        </div>

                        <div class="code-example">
                            <h4>Detailed Classification Report</h4>
                            <pre><code class="language-csharp">// Using visualization assistant for detailed metrics
if (mlManager is PythonMLManager manager)
{
    // Generate comprehensive evaluation report
    manager.Visualization.GenerateEvaluationReport(
        "customer_churn_model", 
        "outputs/evaluation_report.html");
    
    // Create individual visualizations
    manager.Visualization.CreateROC("customer_churn_model", "outputs/roc_curve.png");
    manager.Visualization.CreateConfusionMatrix("customer_churn_model", "outputs/confusion_matrix.png");
    manager.Visualization.CreatePrecisionRecallCurve("customer_churn_model", "outputs/pr_curve.png");
    
    Console.WriteLine("? Classification evaluation completed!");
}</code></pre>
                        </div>
                    </div>

                    <div class="method">
                        <h3><i class="bi bi-graph-up"></i> ROC Curve Analysis</h3>
                        
                        <div class="code-example">
                            <h4>ROC Curve Generation</h4>
                            <pre><code class="language-csharp">// Create ROC curve for binary classification
if (mlManager is PythonMLManager manager)
{
    bool rocCreated = manager.Visualization.CreateROC(
        modelId: "binary_classifier",
        imagePath: "outputs/roc_analysis.png");
    
    if (rocCreated)
    {
        Console.WriteLine("ROC curve saved to outputs/roc_analysis.png");
        
        // ROC curve shows:
        // - True Positive Rate vs False Positive Rate
        // - AUC (Area Under Curve) value
        // - Optimal threshold point
        // - Comparison with random classifier baseline
    }
}</code></pre>
                        </div>

                        <div class="tip">
                            <h5><i class="bi bi-lightbulb"></i> ROC Curve Interpretation</h5>
                            <ul>
                                <li><strong>AUC = 1.0:</strong> Perfect classifier</li>
                                <li><strong>AUC = 0.9-1.0:</strong> Excellent performance</li>
                                <li><strong>AUC = 0.8-0.9:</strong> Good performance</li>
                                <li><strong>AUC = 0.7-0.8:</strong> Fair performance</li>
                                <li><strong>AUC = 0.5:</strong> Random classifier (no predictive value)</li>
                                <li><strong>AUC < 0.5:</strong> Worse than random (may indicate label issues)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="method">
                        <h3><i class="bi bi-grid-3x3"></i> Confusion Matrix Analysis</h3>
                        
                        <div class="code-example">
                            <h4>Confusion Matrix Generation</h4>
                            <pre><code class="language-csharp">// Create detailed confusion matrix
if (mlManager is PythonMLManager manager)
{
    bool matrixCreated = manager.Visualization.CreateConfusionMatrix(
        modelId: "multiclass_classifier",
        imagePath: "outputs/confusion_matrix.png");
    
    if (matrixCreated)
    {
        Console.WriteLine("Confusion matrix analysis completed");
        
        // Confusion matrix provides:
        // - True vs Predicted class distributions
        // - Per-class precision and recall
        // - Identification of commonly confused classes
        // - Overall model accuracy breakdown
    }
}</code></pre>
                        </div>

                        <div class="code-example">
                            <h4>Interpreting Confusion Matrix Results</h4>
                            <pre><code class="language-csharp">// Example confusion matrix interpretation
/*
Confusion Matrix for 3-class problem:
                 Predicted
              A    B    C
Actual   A   85    3    2    (Precision A: 85/90 = 94.4%)
         B    4   76    5    (Precision B: 76/85 = 89.4%)  
         C    1    6   83    (Precision C: 83/90 = 92.2%)
         
Recall A: 85/90 = 94.4%
Recall B: 76/85 = 89.4%
Recall C: 83/90 = 92.2%

Overall Accuracy: (85+76+83)/270 = 90.4%
*/

// Key insights:
// 1. Class B has slightly lower precision (more false positives)
// 2. Class B also has lower recall (more false negatives)
// 3. Classes A and C perform similarly well
// 4. Main confusion is between classes B and C (6 misclassifications)</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Regression Evaluation -->
                <section class="section" id="regression-evaluation">
                    <h2>?? Regression Model Evaluation</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-graph-down"></i> Regression Metrics</h3>
                        
                        <div class="code-example">
                            <h4>Get Regression Scores</h4>
                            <pre><code class="language-csharp">// Get comprehensive regression metrics
var regressionScores = mlManager.GetModelRegressionScores("house_price_model");
double r2Score = regressionScores.Item1;      // R coefficient of determination
double mse = regressionScores.Item2;          // Mean Squared Error
double mae = regressionScores.Item3;          // Mean Absolute Error

Console.WriteLine($"Regression Model Performance:");
Console.WriteLine($"R Score: {r2Score:F4}");
Console.WriteLine($"MSE: {mse:F2}");
Console.WriteLine($"MAE: {mae:F2}");
Console.WriteLine($"RMSE: {Math.Sqrt(mse):F2}");

// Example output:
// Regression Model Performance:
// R Score: 0.8456
// MSE: 1234567.89
// MAE: 85432.10
// RMSE: 1111.11</code></pre>
                        </div>

                        <div class="code-example">
                            <h4>Residual Analysis</h4>
                            <pre><code class="language-csharp">// Create residual plots for regression analysis
if (mlManager is PythonMLManager manager)
{
    // Generate residual analysis plots
    manager.Visualization.CreateLearningCurve(
        "house_price_model", 
        "outputs/learning_curve.png");
    
    // Additional residual analysis can be done with custom Python scripts
    // through the PythonScriptTemplateManager
    
    Console.WriteLine("Residual analysis completed");
    
    // Residual plots help identify:
    // - Homoscedasticity (constant variance)
    // - Linearity assumptions
    // - Outliers and influential points
    // - Model adequacy
}</code></pre>
                        </div>
                    </div>

                    <div class="method">
                        <h3><i class="bi bi-speedometer2"></i> Performance Interpretation</h3>
                        
                        <div class="tip">
                            <h5><i class="bi bi-lightbulb"></i> Regression Metrics Interpretation</h5>
                            <ul>
                                <li><strong>R Score:</strong>
                                    <ul>
                                        <li>1.0 = Perfect fit</li>
                                        <li>0.9+ = Excellent model</li>
                                        <li>0.7-0.9 = Good model</li>
                                        <li>0.5-0.7 = Moderate model</li>
                                        <li>&lt;0.5 = Poor model</li>
                                    </ul>
                                </li>
                                <li><strong>RMSE vs MAE:</strong> If RMSE >> MAE, large errors are present</li>
                                <li><strong>Context Matters:</strong> Compare metrics to business requirements</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Cross-Validation -->
                <section class="section" id="cross-validation">
                    <h2>?? Cross-Validation Evaluation</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-arrow-repeat"></i> K-Fold Cross-Validation</h3>
                        
                        <div class="code-example">
                            <h4>Perform Cross-Validation</h4>
                            <pre><code class="language-csharp">// Perform k-fold cross-validation for robust evaluation
if (mlManager is PythonMLManager manager)
{
    // 10-fold cross-validation
    manager.CrossValidation.PerformCrossValidation(
        modelId: "robust_classifier",
        numFolds: 10);
    
    Console.WriteLine("Cross-validation completed");
    
    // Cross-validation provides:
    // - Mean CV score across all folds
    // - Standard deviation of scores
    // - Confidence intervals
    // - Variance in model performance
}</code></pre>
                        </div>

                        <div class="code-example">
                            <h4>Stratified Cross-Validation</h4>
                            <pre><code class="language-csharp">// Use stratified sampling for imbalanced datasets
if (mlManager is PythonMLManager manager)
{
    // Stratified sampling maintains class distribution in each fold
    manager.CrossValidation.PerformStratifiedSampling(
        testSize: 0.2f,
        trainFilePath: "train_stratified.csv",
        testFilePath: "test_stratified.csv");
    
    // Then perform cross-validation
    manager.CrossValidation.PerformCrossValidation("stratified_model", 5);
    
    Console.WriteLine("Stratified cross-validation maintains class balance");
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Feature Importance -->
                <section class="section" id="feature-importance">
                    <h2>?? Feature Importance Analysis</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-bar-chart-line"></i> Feature Importance Visualization</h3>
                        
                        <div class="code-example">
                            <h4>Create Feature Importance Plot</h4>
                            <pre><code class="language-csharp">// Generate feature importance analysis
if (mlManager is PythonMLManager manager)
{
    bool importanceCreated = manager.Visualization.CreateFeatureImportance(
        modelId: "feature_analysis_model",
        imagePath: "outputs/feature_importance.png");
    
    if (importanceCreated)
    {
        Console.WriteLine("Feature importance analysis completed");
        
        // Feature importance plot shows:
        // - Relative importance of each feature
        // - Which features contribute most to predictions
        // - Potential candidates for feature selection
        // - Model interpretability insights
    }
}</code></pre>
                        </div>

                        <div class="tip">
                            <h5><i class="bi bi-lightbulb"></i> Feature Importance Best Practices</h5>
                            <ul>
                                <li><strong>Tree-based models:</strong> Natural feature importance (Gini, entropy)</li>
                                <li><strong>Linear models:</strong> Coefficient magnitudes (after scaling)</li>
                                <li><strong>Permutation importance:</strong> Model-agnostic approach</li>
                                <li><strong>SHAP values:</strong> Advanced explainable AI technique</li>
                                <li><strong>Feature selection:</strong> Remove low-importance features</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Model Comparison -->
                <section class="section" id="model-comparison">
                    <h2>?? Model Comparison and Selection</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-graph-up"></i> Multiple Model Evaluation</h3>
                        
                        <div class="code-example">
                            <h4>Compare Multiple Models</h4>
                            <pre><code class="language-csharp">public async Task CompareMultipleModels()
{
    var models = new Dictionary&lt;string, (MachineLearningAlgorithm Algorithm, Dictionary&lt;string, object&gt; Parameters)&gt;
    {
        ["RandomForest"] = (MachineLearningAlgorithm.RandomForestClassifier, new Dictionary&lt;string, object&gt;
        {
            ["n_estimators"] = 100,
            ["max_depth"] = 10,
            ["random_state"] = 42
        }),
        ["GradientBoosting"] = (MachineLearningAlgorithm.GradientBoostingClassifier, new Dictionary&lt;string, object&gt;
        {
            ["n_estimators"] = 100,
            ["learning_rate"] = 0.1,
            ["random_state"] = 42
        }),
        ["LogisticRegression"] = (MachineLearningAlgorithm.LogisticRegression, new Dictionary&lt;string, object&gt;
        {
            ["C"] = 1.0,
            ["max_iter"] = 1000,
            ["random_state"] = 42
        })
    };

    var results = new List&lt;(string ModelName, double Accuracy, double F1Score)&gt;();
    
    foreach (var model in models)
    {
        Console.WriteLine($"Training and evaluating {model.Key}...");
        
        var modelId = $"comparison_{model.Key.ToLower()}";
        
        // Train model
        mlManager.TrainModel(
            modelId,
            model.Value.Algorithm,
            model.Value.Parameters,
            features,
            "target");
        
        // Evaluate model
        var scores = mlManager.GetModelClassificationScore(modelId);
        results.Add((model.Key, scores.Item1, scores.Item2));
        
        // Perform cross-validation for robust comparison
        if (mlManager is PythonMLManager manager)
        {
            manager.CrossValidation.PerformCrossValidation(modelId, 5);
        }
    }

    // Display comparison results
    Console.WriteLine("\nModel Comparison Results:");
    Console.WriteLine("Model                Accuracy    F1-Score");
    Console.WriteLine("=====================================");
    
    foreach (var result in results.OrderByDescending(r => r.F1Score))
    {
        Console.WriteLine($"{result.ModelName,-15}     {result.Accuracy:P2}      {result.F1Score:F4}");
    }
    
    var bestModel = results.OrderByDescending(r => r.F1Score).First();
    Console.WriteLine($"\n?? Best performing model: {bestModel.ModelName}");
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Advanced Evaluation -->
                <section class="section" id="advanced-evaluation">
                    <h2>?? Advanced Evaluation Techniques</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-cpu"></i> Learning Curves</h3>
                        
                        <div class="code-example">
                            <h4>Generate Learning Curves</h4>
                            <pre><code class="language-csharp">// Create learning curves to analyze model performance vs training size
if (mlManager is PythonMLManager manager)
{
    bool learningCurveCreated = manager.Visualization.CreateLearningCurve(
        modelId: "learning_analysis_model",
        imagePath: "outputs/learning_curve.png");
    
    if (learningCurveCreated)
    {
        Console.WriteLine("Learning curve analysis completed");
        
        // Learning curves help identify:
        // - Overfitting (training score >> validation score)
        // - Underfitting (both scores are low)
        // - Optimal training set size
        // - Whether more data would help
    }
}</code></pre>
                        </div>
                    </div>

                    <div class="method">
                        <h3><i class="bi bi-file-text"></i> Comprehensive Evaluation Report</h3>
                        
                        <div class="code-example">
                            <h4>Generate Complete Evaluation Report</h4>
                            <pre><code class="language-csharp">// Generate comprehensive HTML evaluation report
if (mlManager is PythonMLManager manager)
{
    bool reportGenerated = manager.Visualization.GenerateEvaluationReport(
        modelId: "comprehensive_model",
        outputHtmlPath: "reports/complete_evaluation.html");
    
    if (reportGenerated)
    {
        Console.WriteLine("Comprehensive evaluation report generated");
        
        // Report includes:
        // - All performance metrics
        // - Confusion matrix
        // - ROC curve and AUC
        // - Precision-recall curve
        // - Feature importance
        // - Cross-validation results
        // - Model parameters
        // - Training details
    }
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Best Practices -->
                <section class="section" id="evaluation-best-practices">
                    <h2>? Evaluation Best Practices</h2>
                    
                    <div class="tip">
                        <h5><i class="bi bi-lightbulb"></i> Model Evaluation Guidelines</h5>
                        <ul>
                            <li><strong>Use Multiple Metrics:</strong> No single metric tells the complete story</li>
                            <li><strong>Cross-Validation:</strong> Essential for robust performance estimates</li>
                            <li><strong>Business Context:</strong> Align metrics with business objectives</li>
                            <li><strong>Baseline Comparison:</strong> Compare against simple baseline models</li>
                            <li><strong>Statistical Significance:</strong> Test if improvements are meaningful</li>
                            <li><strong>Hold-out Test Set:</strong> Final evaluation on completely unseen data</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h5><i class="bi bi-exclamation-triangle"></i> Common Evaluation Pitfalls</h5>
                        <ul>
                            <li><strong>Data Leakage:</strong> Information from future/target leaking into features</li>
                            <li><strong>Selection Bias:</strong> Choosing metrics that favor your model</li>
                            <li><strong>Overfitting to Validation:</strong> Too much hyperparameter tuning</li>
                            <li><strong>Imbalanced Classes:</strong> Accuracy can be misleading</li>
                            <li><strong>Temporal Issues:</strong> Using future data to predict the past</li>
                        </ul>
                    </div>
                </section>

                <!-- Navigation Links -->
                <div class="nav-links">
                    <a href="algorithm-management.html" class="btn-beep">
                        <i class="bi bi-cpu"></i> Algorithm Management
                    </a>
                    <a href="training-workflows.html" class="btn-beep">
                        <i class="bi bi-workflow"></i> Training Workflows
                    </a>
                    <a href="hyperparameter-tuning.html" class="btn-beep">
                        <i class="bi bi-sliders"></i> Hyperparameter Tuning
                    </a>
                </div>

                <!-- Footer -->
                <footer class="documentation-footer">
                    <div class="footer-content">
                        <div class="footer-copyright">
                            <p>&copy; 2024 The Tech Idea - Beep.Python.ML Model Evaluation Documentation</p>
                            <p>Comprehensive Model Evaluation for .NET 6, 7, 8, and 9</p>
                        </div>
                        <div class="footer-links">
                            <a href="#evaluation-overview">Evaluation Overview</a>
                            <a href="#classification-evaluation">Classification</a>
                            <a href="#regression-evaluation">Regression</a>
                            <a href="#cross-validation">Cross-Validation</a>
                        </div>
                    </div>
                </footer>
            </div>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="assets/navigation.js"></script>
</body>
</html>