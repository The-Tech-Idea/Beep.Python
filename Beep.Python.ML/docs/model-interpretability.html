<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Interpretability - Beep.Python.ML</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <link href="assets/styles.css" rel="stylesheet">
</head>
<body>
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">
        <i class="bi bi-list"></i>
    </button>

    <!-- Theme Toggle -->
    <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
        <i class="bi bi-sun-fill" id="theme-icon"></i>
    </button>

    <div class="container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <!-- Navigation will be loaded dynamically -->
        </aside>

        <!-- Main Content -->
        <main class="content">
            <div class="content-wrapper">
                <!-- Page Header -->
                <div class="page-header">
                    <div class="header-content">
                        <h1><i class="bi bi-eye text-primary"></i> Model Interpretability</h1>
                        <p class="lead">Advanced model interpretability and explainability techniques for understanding ML model decisions in .NET 6, 7, 8, and 9</p>
                        
                        <div class="achievement-badges">
                            <span class="badge bg-success"><i class="bi bi-check-circle"></i> SHAP Analysis</span>
                            <span class="badge bg-info"><i class="bi bi-graph-up"></i> LIME Explanations</span>
                            <span class="badge bg-warning"><i class="bi bi-eye"></i> Visual Insights</span>
                            <span class="badge bg-primary"><i class="bi bi-star"></i> Trust & Transparency</span>
                        </div>
                    </div>
                </div>

                <!-- Interpretability Overview -->
                <section class="section" id="interpretability-overview">
                    <h2>?? Model Interpretability Overview</h2>
                    <p>Model interpretability is crucial for understanding how machine learning models make decisions, ensuring trust, compliance, and debugging. Beep.Python.ML provides comprehensive tools for model explanation and interpretability analysis.</p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <div class="feature-card">
                                <h4><i class="bi bi-lightbulb text-primary"></i> Global Interpretability</h4>
                                <ul>
                                    <li><strong>Feature Importance:</strong> Overall feature contributions</li>
                                    <li><strong>Partial Dependence:</strong> Feature effects on predictions</li>
                                    <li><strong>SHAP Summary:</strong> Global feature impact analysis</li>
                                    <li><strong>Model Behavior:</strong> Decision boundary visualization</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="col-md-6">
                            <div class="feature-card">
                                <h4><i class="bi bi-search text-success"></i> Local Interpretability</h4>
                                <ul>
                                    <li><strong>LIME:</strong> Local instance explanations</li>
                                    <li><strong>SHAP Values:</strong> Additive feature attributions</li>
                                    <li><strong>Counterfactuals:</strong> What-if scenarios</li>
                                    <li><strong>Anchors:</strong> High-precision explanations</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- SHAP Integration -->
                <section class="section" id="shap-integration">
                    <h2>?? SHAP (SHapley Additive exPlanations)</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-diagram-2"></i> SHAP Analysis Implementation</h3>
                        
                        <div class="code-example">
                            <h4>Comprehensive SHAP Integration</h4>
                            <pre><code class="language-csharp">// SHAP analysis using PythonVisualizationAssistant
public async Task&lt;SHAPAnalysisResult&gt; PerformSHAPAnalysis(string modelId, SHAPConfiguration config)
{
    try
    {
        if (MLManager?.Visualization == null)
            throw new InvalidOperationException("Visualization assistant not available");

        var result = new SHAPAnalysisResult { ModelId = modelId, StartTime = DateTime.Now };
        
        // Generate SHAP values
        var shapValues = await MLManager.Visualization.GenerateSHAPValues(
            modelId: modelId,
            analysisType: config.AnalysisType,
            maxDisplay: config.MaxFeatures,
            outputPath: config.OutputPath);

        if (shapValues.Success)
        {
            result.Success = true;
            result.GlobalImportance = shapValues.GlobalImportance;
            result.LocalExplanations = shapValues.LocalExplanations;
            result.BaselineValue = shapValues.BaselineValue;

            // Generate different SHAP visualizations
            await GenerateSHAPVisualizations(shapValues, config);
            result.GeneratedVisualizations = GetGeneratedVisualizationPaths(config.OutputPath);
        }
        else
        {
            result.Success = false;
            result.ErrorMessage = shapValues.ErrorMessage;
        }

        result.EndTime = DateTime.Now;
        result.Duration = result.EndTime - result.StartTime;

        return result;
    }
    catch (Exception ex)
    {
        return new SHAPAnalysisResult
        {
            Success = false,
            ErrorMessage = ex.Message,
            ModelId = modelId
        };
    }
}

public class SHAPConfiguration
{
    public string AnalysisType { get; set; } = "summary"; // "summary", "waterfall", "force", "dependence"
    public int MaxFeatures { get; set; } = 20;
    public string OutputPath { get; set; } = "shap_analysis";
    public bool GenerateSummaryPlot { get; set; } = true;
    public bool GenerateWaterfallPlot { get; set; } = true;
    public bool GenerateForcePlot { get; set; } = false;
    public bool GenerateDependencePlot { get; set; } = true;
    public string[] FeaturesToAnalyze { get; set; }
    public bool SaveInteractiveHTML { get; set; } = true;
}

private async Task GenerateSHAPVisualizations(SHAPResult shapValues, SHAPConfiguration config)
{
    var tasks = new List&lt;Task&gt;();

    // Summary plot - shows feature importance for all samples
    if (config.GenerateSummaryPlot)
    {
        tasks.Add(MLManager.Visualization.ExecutePythonScript(@"
import shap
import matplotlib.pyplot as plt

# Generate SHAP summary plot
shap.summary_plot(shap_values, X_test, feature_names=feature_names, 
                  max_display=" + config.MaxFeatures + @", show=False)
plt.tight_layout()
plt.savefig('" + Path.Combine(config.OutputPath, "shap_summary.png") + @"', 
            dpi=300, bbox_inches='tight')
plt.close()
"));
    }

    // Waterfall plot - shows how each feature contributes to a single prediction
    if (config.GenerateWaterfallPlot)
    {
        tasks.Add(MLManager.Visualization.ExecutePythonScript(@"
# Generate waterfall plot for first instance
shap.waterfall_plot(shap.Explanation(values=shap_values[0], 
                                     base_values=expected_value,
                                     data=X_test.iloc[0], 
                                     feature_names=feature_names),
                   max_display=" + config.MaxFeatures + @", show=False)
plt.tight_layout()
plt.savefig('" + Path.Combine(config.OutputPath, "shap_waterfall.png") + @"', 
            dpi=300, bbox_inches='tight')
plt.close()
"));
    }

    // Dependence plot - shows the effect of a single feature across the dataset
    if (config.GenerateDependencePlot && config.FeaturesToAnalyze?.Any() == true)
    {
        foreach (var feature in config.FeaturesToAnalyze.Take(3)) // Limit to top 3 features
        {
            tasks.Add(MLManager.Visualization.ExecutePythonScript($@"
# Generate dependence plot for {feature}
shap.dependence_plot('{feature}', shap_values, X_test,
                    feature_names=feature_names, show=False)
plt.tight_layout()
plt.savefig('{Path.Combine(config.OutputPath, $"shap_dependence_{feature}.png")}', 
            dpi=300, bbox_inches='tight')
plt.close()
"));
        }
    }

    // Force plot - interactive HTML explanation
    if (config.SaveInteractiveHTML)
    {
        tasks.Add(MLManager.Visualization.ExecutePythonScript(@"
# Generate interactive force plot
force_plot = shap.force_plot(expected_value, shap_values[0], X_test.iloc[0],
                             feature_names=feature_names, matplotlib=False)
shap.save_html('" + Path.Combine(config.OutputPath, "shap_force_plot.html") + @"', force_plot)
"));
    }

    await Task.WhenAll(tasks);
}</code></pre>
                        </div>
                    </div>

                    <div class="method">
                        <h3><i class="bi bi-bar-chart-fill"></i> SHAP Feature Analysis</h3>
                        
                        <div class="code-example">
                            <h4>Advanced SHAP Feature Impact Analysis</h4>
                            <pre><code class="language-csharp">public class SHAPFeatureAnalysis
{
    public async Task&lt;FeatureImpactReport&gt; AnalyzeFeatureImpacts(string modelId, 
        string[] instances = null)
    {
        var report = new FeatureImpactReport();
        
        try
        {
            // Get SHAP values for analysis
            var shapResult = await MLManager.Visualization.GenerateSHAPValues(
                modelId: modelId,
                analysisType: "detailed",
                instances: instances);

            if (!shapResult.Success)
            {
                report.Success = false;
                report.ErrorMessage = shapResult.ErrorMessage;
                return report;
            }

            // Analyze global feature importance
            report.GlobalFeatureImportance = CalculateGlobalImportance(shapResult.Values);
            
            // Analyze feature interactions
            report.FeatureInteractions = await AnalyzeFeatureInteractions(shapResult.Values);
            
            // Identify most influential features per class (for classification)
            if (IsClassificationModel(modelId))
            {
                report.ClassSpecificImportance = AnalyzeClassSpecificImportance(shapResult.Values);
            }
            
            // Generate feature impact summaries
            report.FeatureImpactSummaries = GenerateFeatureImpactSummaries(shapResult.Values);
            
            // Calculate feature stability (consistency across predictions)
            report.FeatureStability = CalculateFeatureStability(shapResult.Values);

            report.Success = true;
            return report;
        }
        catch (Exception ex)
        {
            report.Success = false;
            report.ErrorMessage = ex.Message;
            return report;
        }
    }

    private Dictionary&lt;string, double&gt; CalculateGlobalImportance(double[,] shapValues)
    {
        var importance = new Dictionary&lt;string, double&gt;();
        
        // Calculate mean absolute SHAP values for each feature
        for (int featureIndex = 0; featureIndex < shapValues.GetLength(1); featureIndex++)
        {
            double sum = 0;
            int count = shapValues.GetLength(0);
            
            for (int instanceIndex = 0; instanceIndex < count; instanceIndex++)
            {
                sum += Math.Abs(shapValues[instanceIndex, featureIndex]);
            }
            
            var featureName = GetFeatureName(featureIndex);
            importance[featureName] = sum / count;
        }
        
        return importance.OrderByDescending(kvp => kvp.Value)
                        .ToDictionary(kvp => kvp.Key, kvp => kvp.Value);
    }

    private async Task&lt;Dictionary&lt;string, FeatureInteractionInfo&gt;&gt; AnalyzeFeatureInteractions(
        double[,] shapValues)
    {
        var interactions = new Dictionary&lt;string, FeatureInteractionInfo&gt;();
        
        // Use SHAP interaction values if available
        var interactionScript = @"
import shap
import numpy as np

# Calculate SHAP interaction values
interaction_values = explainer.shap_interaction_values(X_test)

# Find top feature interactions
feature_count = len(feature_names)
interaction_strengths = []

for i in range(feature_count):
    for j in range(i+1, feature_count):
        # Calculate interaction strength
        interaction_strength = np.abs(interaction_values[:, i, j]).mean()
        interaction_strengths.append({
            'feature1': feature_names[i],
            'feature2': feature_names[j], 
            'strength': interaction_strength
        })

# Sort by interaction strength
interaction_strengths.sort(key=lambda x: x['strength'], reverse=True)
top_interactions = interaction_strengths[:10]  # Top 10 interactions
";

        var result = await MLManager.Visualization.ExecutePythonScript(interactionScript);
        
        // Parse results and populate interactions dictionary
        // Implementation would parse the Python output and populate the dictionary
        
        return interactions;
    }

    public class FeatureImpactReport
    {
        public bool Success { get; set; }
        public string ErrorMessage { get; set; }
        public Dictionary&lt;string, double&gt; GlobalFeatureImportance { get; set; }
        public Dictionary&lt;string, FeatureInteractionInfo&gt; FeatureInteractions { get; set; }
        public Dictionary&lt;string, Dictionary&lt;string, double&gt;&gt; ClassSpecificImportance { get; set; }
        public Dictionary&lt;string, FeatureImpactSummary&gt; FeatureImpactSummaries { get; set; }
        public Dictionary&lt;string, double&gt; FeatureStability { get; set; }
        public DateTime GeneratedAt { get; set; } = DateTime.Now;
    }

    public class FeatureInteractionInfo
    {
        public string Feature1 { get; set; }
        public string Feature2 { get; set; }
        public double InteractionStrength { get; set; }
        public string InteractionType { get; set; } // "Synergistic", "Redundant", "Independent"
        public double[] InteractionValues { get; set; }
    }

    public class FeatureImpactSummary
    {
        public string FeatureName { get; set; }
        public double MeanAbsoluteImpact { get; set; }
        public double PositiveImpact { get; set; }
        public double NegativeImpact { get; set; }
        public double ImpactVariance { get; set; }
        public string[] TopPositiveInstances { get; set; }
        public string[] TopNegativeInstances { get; set; }
        public string ImpactDescription { get; set; }
    }
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- LIME Integration -->
                <section class="section" id="lime-integration">
                    <h2>?? LIME (Local Interpretable Model-agnostic Explanations)</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-zoom-in"></i> LIME Local Explanations</h3>
                        
                        <div class="code-example">
                            <h4>LIME Implementation for Local Interpretability</h4>
                            <pre><code class="language-csharp">// LIME local explanation generation
public async Task&lt;LIMEExplanationResult&gt; GenerateLIMEExplanation(
    string modelId, 
    int instanceIndex, 
    LIMEConfiguration config)
{
    try
    {
        var result = new LIMEExplanationResult 
        { 
            ModelId = modelId, 
            InstanceIndex = instanceIndex,
            StartTime = DateTime.Now 
        };

        // Determine data type and use appropriate LIME explainer
        var dataType = await DetermineDataType(modelId);
        
        string limeScript = dataType switch
        {
            "tabular" => GenerateTabularLIMEScript(instanceIndex, config),
            "text" => GenerateTextLIMEScript(instanceIndex, config),
            "image" => GenerateImageLIMEScript(instanceIndex, config),
            _ => throw new NotSupportedException($"Data type {dataType} not supported for LIME")
        };

        var limeResult = await MLManager.Visualization.ExecutePythonScript(limeScript);
        
        if (limeResult.Success)
        {
            result.Success = true;
            result.Explanation = ParseLIMEExplanation(limeResult.Output);
            result.FeatureImportances = ExtractFeatureImportances(limeResult.Output);
            result.LocalPrediction = ExtractLocalPrediction(limeResult.Output);
            result.VisualizationPath = Path.Combine(config.OutputPath, $"lime_explanation_{instanceIndex}.png");
        }
        
        result.EndTime = DateTime.Now;
        result.Duration = result.EndTime - result.StartTime;
        
        return result;
    }
    catch (Exception ex)
    {
        return new LIMEExplanationResult
        {
            Success = false,
            ErrorMessage = ex.Message,
            ModelId = modelId,
            InstanceIndex = instanceIndex
        };
    }
}

private string GenerateTabularLIMEScript(int instanceIndex, LIMEConfiguration config)
{
    return $@"
import lime
import lime.lime_tabular
import numpy as np
import matplotlib.pyplot as plt

# Initialize LIME explainer for tabular data
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns,
    class_names=class_names if hasattr(model, 'classes_') else None,
    mode='classification' if hasattr(model, 'classes_') else 'regression',
    discretize_continuous={str(config.DiscretizeContinuous).lower()},
    random_state=42
)

# Generate explanation for the specified instance
instance = X_test.iloc[{instanceIndex}]
explanation = explainer.explain_instance(
    data_row=instance.values,
    predict_fn=model.predict_proba if hasattr(model, 'predict_proba') else model.predict,
    num_features={config.NumFeatures},
    num_samples={config.NumSamples}
)

# Save explanation as image
fig = explanation.as_pyplot_figure()
fig.tight_layout()
fig.savefig('{Path.Combine(config.OutputPath, $"lime_explanation_{instanceIndex}.png")}', 
            dpi=300, bbox_inches='tight')
plt.close()

# Extract explanation data
explanation_data = {{
    'instance_index': {instanceIndex},
    'prediction': explanation.predicted_value if hasattr(explanation, 'predicted_value') else None,
    'features': explanation.as_list(),
    'local_prediction': explanation.local_pred[0] if hasattr(explanation, 'local_pred') else None,
    'intercept': explanation.intercept[0] if hasattr(explanation, 'intercept') else None
}}

print('LIME_RESULT:', explanation_data)
";
}

public class LIMEConfiguration
{
    public int NumFeatures { get; set; } = 10;
    public int NumSamples { get; set; } = 5000;
    public bool DiscretizeContinuous { get; set; } = true;
    public string OutputPath { get; set; } = "lime_explanations";
    public string Mode { get; set; } = "auto"; // "classification", "regression", "auto"
}

public class LIMEExplanationResult
{
    public bool Success { get; set; }
    public string ErrorMessage { get; set; }
    public string ModelId { get; set; }
    public int InstanceIndex { get; set; }
    public Dictionary&lt;string, double&gt; FeatureImportances { get; set; }
    public string Explanation { get; set; }
    public double LocalPrediction { get; set; }
    public string VisualizationPath { get; set; }
    public DateTime StartTime { get; set; }
    public DateTime EndTime { get; set; }
    public TimeSpan Duration { get; set; }
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Counterfactual Explanations -->
                <section class="section" id="counterfactual-explanations">
                    <h2>?? Counterfactual Explanations</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-arrow-left-right"></i> What-If Analysis</h3>
                        
                        <div class="code-example">
                            <h4>Counterfactual Generation</h4>
                            <pre><code class="language-csharp">// Generate counterfactual explanations
public async Task&lt;CounterfactualResult&gt; GenerateCounterfactuals(
    string modelId, 
    int instanceIndex, 
    CounterfactualConfiguration config)
{
    try
    {
        var result = new CounterfactualResult
        {
            ModelId = modelId,
            InstanceIndex = instanceIndex,
            StartTime = DateTime.Now
        };

        // Generate counterfactuals using DiCE (Diverse Counterfactual Explanations)
        var counterfactualScript = $@"
import dice_ml
import pandas as pd
import numpy as np

# Initialize DiCE model
dice_data = dice_ml.Data(dataframe=df_train, 
                        continuous_features=continuous_features,
                        outcome_name='{config.TargetColumn}')

dice_model = dice_ml.Model(model=model, backend='sklearn')
dice_explainer = dice_ml.Dice(dice_data, dice_model, method='{config.Method}')

# Get the instance to explain
query_instance = X_test.iloc[{instanceIndex}].to_dict()

# Generate counterfactuals
counterfactuals = dice_explainer.generate_counterfactuals(
    query_instances=pd.DataFrame([query_instance]),
    total_CFs={config.NumCounterfactuals},
    desired_class={config.DesiredClass if config.DesiredClass != null else "opposite"},
    proximity_weight={config.ProximityWeight},
    diversity_weight={config.DiversityWeight},
    categorical_penalty={config.CategoricalPenalty}
)

# Extract counterfactual data
cf_examples = counterfactuals.cf_examples_list[0]
cf_data = {{
    'original_instance': query_instance,
    'counterfactuals': cf_examples.to_dict('records'),
    'feature_changes': [],
    'distances': []
}}

# Calculate feature changes and distances
original_values = pd.Series(query_instance)
for idx, cf_row in cf_examples.iterrows():
    changes = {{}}
    distance = 0
    
    for feature in original_values.index:
        if original_values[feature] != cf_row[feature]:
            changes[feature] = {{
                'original': original_values[feature],
                'counterfactual': cf_row[feature],
                'change': cf_row[feature] - original_values[feature] if pd.api.types.is_numeric_dtype(type(original_values[feature])) else 'categorical_change'
            }}
            
        # Calculate L2 distance
        if pd.api.types.is_numeric_dtype(type(original_values[feature])):
            distance += (original_values[feature] - cf_row[feature]) ** 2
    
    cf_data['feature_changes'].append(changes)
    cf_data['distances'].append(np.sqrt(distance))

print('COUNTERFACTUAL_RESULT:', cf_data)
";

        var cfResult = await MLManager.Visualization.ExecutePythonScript(counterfactualScript);
        
        if (cfResult.Success)
        {
            result.Success = true;
            result.Counterfactuals = ParseCounterfactuals(cfResult.Output);
            result.FeatureChanges = ExtractFeatureChanges(cfResult.Output);
            result.Distances = ExtractDistances(cfResult.Output);
            
            // Generate visualization
            await GenerateCounterfactualVisualization(result, config);
        }

        result.EndTime = DateTime.Now;
        result.Duration = result.EndTime - result.StartTime;
        
        return result;
    }
    catch (Exception ex)
    {
        return new CounterfactualResult
        {
            Success = false,
            ErrorMessage = ex.Message,
            ModelId = modelId,
            InstanceIndex = instanceIndex
        };
    }
}

public class CounterfactualConfiguration
{
    public string Method { get; set; } = "random"; // "random", "genetic", "gradient"
    public int NumCounterfactuals { get; set; } = 5;
    public string DesiredClass { get; set; } = null; // null for "opposite"
    public double ProximityWeight { get; set; } = 0.5;
    public double DiversityWeight { get; set; } = 1.0;
    public double CategoricalPenalty { get; set; } = 0.1;
    public string TargetColumn { get; set; }
    public string[] ImmutableFeatures { get; set; } = new string[0];
    public string OutputPath { get; set; } = "counterfactuals";
}

public class CounterfactualResult
{
    public bool Success { get; set; }
    public string ErrorMessage { get; set; }
    public string ModelId { get; set; }
    public int InstanceIndex { get; set; }
    public List&lt;Dictionary&lt;string, object&gt;&gt; Counterfactuals { get; set; }
    public List&lt;Dictionary&lt;string, FeatureChange&gt;&gt; FeatureChanges { get; set; }
    public List&lt;double&gt; Distances { get; set; }
    public string VisualizationPath { get; set; }
    public DateTime StartTime { get; set; }
    public DateTime EndTime { get; set; }
    public TimeSpan Duration { get; set; }
}

public class FeatureChange
{
    public object OriginalValue { get; set; }
    public object CounterfactualValue { get; set; }
    public object Change { get; set; }
    public string ChangeType { get; set; } // "increase", "decrease", "categorical_change"
    public double RelativeChange { get; set; }
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Model Interpretability Dashboard -->
                <section class="section" id="interpretability-dashboard">
                    <h2>?? Interpretability Dashboard Integration</h2>
                    
                    <div class="method">
                        <h3><i class="bi bi-speedometer2"></i> Complete Interpretability ViewModel</h3>
                        
                        <div class="code-example">
                            <h4>MVVM Integration for Model Interpretability</h4>
                            <pre><code class="language-csharp">public partial class ModelInterpretabilityViewModel : PythonBaseViewModel
{
    // Observable properties for interpretability
    [ObservableProperty] private bool isAnalysisRunning;
    [ObservableProperty] private string currentAnalysisType;
    [ObservableProperty] private double analysisProgress;
    
    // SHAP analysis results
    [ObservableProperty] private SHAPAnalysisResult shapResult;
    [ObservableProperty] private ObservableCollection&lt;FeatureImportanceItem&gt; globalFeatureImportance;
    [ObservableProperty] private ObservableCollection&lt;FeatureInteractionItem&gt; featureInteractions;
    
    // LIME analysis results
    [ObservableProperty] private LIMEExplanationResult limeResult;
    [ObservableProperty] private ObservableCollection&lt;LocalExplanationItem&gt; localExplanations;
    
    // Counterfactual analysis
    [ObservableProperty] private CounterfactualResult counterfactualResult;
    [ObservableProperty] private ObservableCollection&lt;CounterfactualInstance&gt; counterfactualExamples;
    
    // Analysis configuration
    [ObservableProperty] private InterpretabilityConfiguration analysisConfig;
    [ObservableProperty] private int selectedInstanceIndex;
    [ObservableProperty] private string selectedFeatureForAnalysis;
    
    // Commands
    public IAsyncRelayCommand RunSHAPAnalysisCommand { get; }
    public IAsyncRelayCommand RunLIMEAnalysisCommand { get; }
    public IAsyncRelayCommand GenerateCounterfactualsCommand { get; }
    public IAsyncRelayCommand RunFullInterpretabilityAnalysisCommand { get; }
    public IRelayCommand ExportInterpretabilityReportCommand { get; }
    
    public ModelInterpretabilityViewModel(IBeepService beepService, 
                                        IPythonNetRunTimeManager pythonRuntime, 
                                        PythonSessionInfo sessionInfo)
        : base(beepService, pythonRuntime, sessionInfo)
    {
        // Initialize collections
        GlobalFeatureImportance = new ObservableCollection&lt;FeatureImportanceItem&gt;();
        FeatureInteractions = new ObservableCollection&lt;FeatureInteractionItem&gt;();
        LocalExplanations = new ObservableCollection&lt;LocalExplanationItem&gt;();
        CounterfactualExamples = new ObservableCollection&lt;CounterfactualInstance&gt;();
        
        // Initialize configuration
        AnalysisConfig = new InterpretabilityConfiguration();
        
        // Initialize commands
        RunSHAPAnalysisCommand = new AsyncRelayCommand(RunSHAPAnalysisAsync, CanRunAnalysis);
        RunLIMEAnalysisCommand = new AsyncRelayCommand(RunLIMEAnalysisAsync, CanRunAnalysis);
        GenerateCounterfactualsCommand = new AsyncRelayCommand(GenerateCounterfactualsAsync, CanRunAnalysis);
        RunFullInterpretabilityAnalysisCommand = new AsyncRelayCommand(RunFullAnalysisAsync, CanRunAnalysis);
        ExportInterpretabilityReportCommand = new RelayCommand(ExportInterpretabilityReport);
    }
    
    private async Task RunSHAPAnalysisAsync()
    {
        try
        {
            IsAnalysisRunning = true;
            CurrentAnalysisType = "SHAP Analysis";
            AnalysisProgress = 0;
            
            var progress = new Progress&lt;double&gt;(value => AnalysisProgress = value);
            
            // Configure SHAP analysis
            var shapConfig = new SHAPConfiguration
            {
                AnalysisType = "summary",
                MaxFeatures = AnalysisConfig.MaxFeatures,
                GenerateSummaryPlot = true,
                GenerateWaterfallPlot = true,
                GenerateDependencePlot = true,
                OutputPath = Path.Combine(AnalysisConfig.OutputDirectory, "shap")
            };
            
            // Run SHAP analysis
            ShapResult = await PerformSHAPAnalysis(CurrentModelId, shapConfig);
            
            if (ShapResult.Success)
            {
                // Update UI collections
                UpdateGlobalFeatureImportance(ShapResult.GlobalImportance);
                
                // Update interactions if available
                if (ShapResult.FeatureInteractions?.Any() == true)
                {
                    UpdateFeatureInteractions(ShapResult.FeatureInteractions);
                }
                
                CurrentAnalysisType = "SHAP Analysis Completed";
                ShowSuccessMessage($"SHAP analysis completed in {ShapResult.Duration.TotalSeconds:F1} seconds");
            }
            else
            {
                ShowErrorMessage($"SHAP analysis failed: {ShapResult.ErrorMessage}");
            }
        }
        catch (Exception ex)
        {
            ShowErrorMessage($"SHAP analysis error: {ex.Message}");
        }
        finally
        {
            IsAnalysisRunning = false;
            AnalysisProgress = 100;
        }
    }
    
    private async Task RunFullAnalysisAsync()
    {
        try
        {
            IsAnalysisRunning = true;
            CurrentAnalysisType = "Complete Interpretability Analysis";
            AnalysisProgress = 0;
            
            // Run all interpretability analyses
            var tasks = new List&lt;Task&gt;
            {
                RunSHAPAnalysisAsync(),
                RunLIMEAnalysisAsync(),
                GenerateCounterfactualsAsync()
            };
            
            var completedTasks = 0;
            foreach (var task in tasks)
            {
                await task;
                completedTasks++;
                AnalysisProgress = (completedTasks * 100.0) / tasks.Count;
            }
            
            // Generate comprehensive report
            await GenerateComprehensiveInterpretabilityReport();
            
            CurrentAnalysisType = "Complete Analysis Finished";
            ShowSuccessMessage("Complete interpretability analysis finished successfully!");
        }
        catch (Exception ex)
        {
            ShowErrorMessage($"Full analysis error: {ex.Message}");
        }
        finally
        {
            IsAnalysisRunning = false;
            AnalysisProgress = 100;
        }
    }
    
    private bool CanRunAnalysis()
    {
        return !IsAnalysisRunning && 
               !string.IsNullOrEmpty(CurrentModelId) && 
               IsMLPipelineReady;
    }
    
    private void UpdateGlobalFeatureImportance(Dictionary&lt;string, double&gt; importance)
    {
        GlobalFeatureImportance.Clear();
        
        foreach (var kvp in importance.OrderByDescending(x => x.Value))
        {
            GlobalFeatureImportance.Add(new FeatureImportanceItem
            {
                FeatureName = kvp.Key,
                Importance = kvp.Value,
                RelativeImportance = kvp.Value / importance.Values.Max(),
                Rank = GlobalFeatureImportance.Count + 1
            });
        }
    }
}

public class InterpretabilityConfiguration
{
    public int MaxFeatures { get; set; } = 20;
    public int NumSamples { get; set; } = 5000;
    public string OutputDirectory { get; set; } = "interpretability_analysis";
    public bool GenerateVisualizations { get; set; } = true;
    public bool SaveInteractiveHTML { get; set; } = true;
    public string[] FeaturesToAnalyze { get; set; }
    public int[] InstancesToExplain { get; set; }
}

public class FeatureImportanceItem
{
    public string FeatureName { get; set; }
    public double Importance { get; set; }
    public double RelativeImportance { get; set; }
    public int Rank { get; set; }
    public string ImportanceDescription => Importance switch
    {
        >= 0.1 => "Very High",
        >= 0.05 => "High",
        >= 0.02 => "Medium",
        >= 0.01 => "Low",
        _ => "Very Low"
    };
}</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Best Practices -->
                <section class="section" id="interpretability-best-practices">
                    <h2>? Interpretability Best Practices</h2>
                    
                    <div class="tip">
                        <h5><i class="bi bi-lightbulb"></i> Model Interpretability Guidelines</h5>
                        <ul>
                            <li><strong>Choose Appropriate Methods:</strong> Use SHAP for global, LIME for local explanations</li>
                            <li><strong>Consider Stakeholders:</strong> Tailor explanations to your audience</li>
                            <li><strong>Validate Explanations:</strong> Ensure explanations align with domain knowledge</li>
                            <li><strong>Multiple Perspectives:</strong> Use different interpretability methods for comprehensive understanding</li>
                            <li><strong>Feature Interactions:</strong> Don't ignore feature interactions in complex models</li>
                            <li><strong>Baseline Comparisons:</strong> Always compare against baseline or simple models</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h5><i class="bi bi-exclamation-triangle"></i> Interpretability Limitations</h5>
                        <ul>
                            <li><strong>Approximations:</strong> LIME and SHAP provide approximations, not exact explanations</li>
                            <li><strong>Local vs Global:</strong> Local explanations don't always generalize globally</li>
                            <li><strong>Computational Cost:</strong> Interpretability analysis can be computationally expensive</li>
                            <li><strong>Feature Dependencies:</strong> Consider feature correlations in interpretations</li>
                            <li><strong>Model Assumptions:</strong> Explanations assume model behavior is meaningful</li>
                        </ul>
                    </div>
                </section>

                <!-- Navigation Links -->
                <div class="nav-links">
                    <a href="model-evaluation.html" class="btn-beep">
                        <i class="bi bi-graph-up"></i> Model Evaluation
                    </a>
                    <a href="ModelEvaluationGraphsViewModel.html" class="btn-beep">
                        <i class="bi bi-bar-chart"></i> Evaluation Graphs
                    </a>
                    <a href="ensemble-methods.html" class="btn-beep">
                        <i class="bi bi-layers"></i> Ensemble Methods
                    </a>
                </div>

                <!-- Footer -->
                <footer class="documentation-footer">
                    <div class="footer-content">
                        <div class="footer-copyright">
                            <p>&copy; 2024 The Tech Idea - Beep.Python.ML Model Interpretability Documentation</p>
                            <p>Advanced Model Explainability for .NET 6, 7, 8, and 9</p>
                        </div>
                        <div class="footer-links">
                            <a href="#interpretability-overview">Overview</a>
                            <a href="#shap-integration">SHAP Analysis</a>
                            <a href="#lime-integration">LIME Explanations</a>
                            <a href="#counterfactual-explanations">Counterfactuals</a>
                        </div>
                    </div>
                </footer>
            </div>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="assets/navigation.js"></script>
</body>
</html>