<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LocalTransformerPipeline - Beep.Python.AI.Transformers API</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <link href="../assets/styles.css" rel="stylesheet">
</head>
<body>
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">
        <i class="bi bi-list"></i>
    </button>

    <!-- Theme Toggle -->
    <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
        <i class="bi bi-sun-fill" id="theme-icon"></i>
    </button>

    <div class="container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <!-- Navigation will be loaded dynamically by navigation.js -->
        </aside>

        <!-- Main Content -->
        <main class="content">
            <div class="content-wrapper">
                <!-- Breadcrumb Navigation -->
                <div class="breadcrumb-nav">
                    <a href="../index.html">Home</a>
                    <span>›</span>
                    <a href="../index.html#providers">AI Providers</a>
                    <span>›</span>
                    <span>Local Models</span>
                </div>

                <!-- Page Header -->
                <div class="page-header">
                    <h1><i class="bi bi-folder"></i> LocalTransformerPipeline</h1>
                    <p class="page-subtitle">Pipeline implementation for locally hosted transformer models with complete data privacy and control</p>
                </div>

                <!-- Class Info -->
                <div class="class-info">
                    <div class="class-namespace">
                        <strong>Namespace:</strong> Beep.Python.AI.Transformers<br>
                        <strong>Assembly:</strong> Beep.Python.AI.Transformers.dll
                    </div>
                    <div class="class-implements">
                        <strong>Inheritance:</strong> BaseTransformerPipeline ? LocalTransformerPipeline<br>
                        <strong>Implements:</strong> ITransformerPipeLine, IDisposable
                    </div>
                </div>

                <!-- Overview -->
                <section class="section" id="overview">
                    <h2>Overview</h2>
                    <p>
                        The <code>LocalTransformerPipeline</code> enables you to run transformer models locally on your 
                        own hardware, providing complete data privacy, no API costs, and full control over the 
                        inference process. It supports various model formats and optimization techniques.
                    </p>
                    
                    <div class="highlight-box bg-light p-4 rounded">
                        <h5><i class="bi bi-lightbulb"></i> Key Advantages</h5>
                        <ul class="mb-0">
                            <li><strong>Complete Privacy</strong> - Your data never leaves your environment</li>
                            <li><strong>No API Costs</strong> - One-time model download, unlimited usage</li>
                            <li><strong>Full Control</strong> - Customize inference parameters and optimizations</li>
                            <li><strong>Offline Capable</strong> - Works without internet connection</li>
                            <li><strong>Enterprise Ready</strong> - Air-gapped deployments supported</li>
                        </ul>
                    </div>
                </section>

                <!-- System Requirements -->
                <section class="section" id="requirements">
                    <h2>System Requirements</h2>
                    
                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4><i class="bi bi-cpu"></i> CPU Requirements</h4>
                            <ul>
                                <li><strong>Minimum:</strong> 4 cores, 8GB RAM</li>
                                <li><strong>Recommended:</strong> 8+ cores, 16GB+ RAM</li>
                                <li><strong>Large Models:</strong> 16+ cores, 32GB+ RAM</li>
                                <li><strong>Architecture:</strong> x64, ARM64 supported</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-gpu-card"></i> GPU Support</h4>
                            <ul>
                                <li><strong>NVIDIA:</strong> CUDA 11.0+ (recommended)</li>
                                <li><strong>AMD:</strong> ROCm support available</li>
                                <li><strong>Apple:</strong> Metal Performance Shaders</li>
                                <li><strong>Intel:</strong> oneAPI DPC++ support</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-hdd"></i> Storage</h4>
                            <ul>
                                <li><strong>Small Models:</strong> 1-5 GB available</li>
                                <li><strong>Medium Models:</strong> 5-15 GB available</li>
                                <li><strong>Large Models:</strong> 15-100+ GB available</li>
                                <li><strong>SSD Recommended:</strong> For faster loading</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-memory"></i> Memory</h4>
                            <ul>
                                <li><strong>7B Models:</strong> 8-16 GB RAM</li>
                                <li><strong>13B Models:</strong> 16-32 GB RAM</li>
                                <li><strong>30B+ Models:</strong> 32-64+ GB RAM</li>
                                <li><strong>GPU VRAM:</strong> 4-24+ GB depending on model</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Configuration -->
                <section class="section" id="configuration">
                    <h2>Configuration</h2>
                    
                    <h3>LocalTransformerConfig</h3>
                    <div class="code-example">
                        <h4>Basic Configuration</h4>
                        <pre><code class="language-csharp">var config = new LocalTransformerConfig
{
    ModelPath = @"C:\Models\llama-2-7b-chat",
    ModelFormat = ModelFormat.GGUF,           // GGUF, GGML, SafeTensors, PyTorch
    Device = ComputeDevice.Auto,              // Auto, CPU, CUDA, Metal
    ContextLength = 4096,
    BatchSize = 8,
    ThreadCount = Environment.ProcessorCount / 2
};</code></pre>
                    </div>

                    <h3>Advanced Configuration</h3>
                    <div class="code-example">
                        <h4>Optimized Setup</h4>
                        <pre><code class="language-csharp">var advancedConfig = new LocalTransformerConfig
{
    ModelPath = @"D:\Models\CodeLlama-13B-Instruct",
    ModelFormat = ModelFormat.GGUF,
    
    // Hardware optimization
    HardwareConfig = new HardwareConfig
    {
        Device = ComputeDevice.CUDA,
        GpuLayers = 40,                       // Offload layers to GPU
        MainGpu = 0,                          // Primary GPU device
        TensorSplit = new[] { 0.7f, 0.3f },   // Multi-GPU distribution
        UseMlock = true,                      // Keep model in RAM
        UseMmap = true,                       // Memory-mapped files
        LowVram = false                       // Enable for limited VRAM
    },
    
    // Performance settings
    PerformanceConfig = new PerformanceConfig
    {
        ContextLength = 8192,
        BatchSize = 16,
        ThreadCount = 12,
        RopeFreqBase = 10000,
        RopeFreqScale = 1.0f
    },
    
    // Quantization options
    QuantizationConfig = new QuantizationConfig
    {
        ModelQuantization = QuantizationType.Q4_K_M,  // Balanced quality/speed
        KvCacheQuantization = QuantizationType.Q8_0,  // Cache quantization
        EnableF16KvCache = true
    },
    
    // Safety and limits
    SafetyConfig = new LocalSafetyConfig
    {
        MaxSequenceLength = 8192,
        TimeoutSeconds = 300,
        EnableContentFiltering = true,
        LogLevel = LogLevel.Information
    }
};</code></pre>
                    </div>
                </section>

                <!-- Usage Examples -->
                <section class="section" id="examples">
                    <h2>Usage Examples</h2>
                    
                    <h3>Basic Local Model Setup</h3>
                    <div class="code-example">
                        <h4>Loading and Using a Local Model</h4>
                        <pre><code class="language-csharp">// Create local pipeline
var config = new LocalTransformerConfig
{
    ModelPath = @"C:\Models\llama-2-7b-chat.gguf",
    ModelFormat = ModelFormat.GGUF,
    Device = ComputeDevice.Auto
};

var pipeline = TransformerPipelineFactory.CreateLocalPipeline(
    runtimeManager, executeManager, config);

await pipeline.InitializeAsync(new TransformerPipelineConfig
{
    TaskType = TransformerTask.TextGeneration
});

// Load the model
await pipeline.LoadModelAsync(new TransformerModelInfo
{
    Name = "llama-2-7b-chat",
    Source = TransformerModelSource.Local,
    LocalPath = config.ModelPath
}, TransformerTask.TextGeneration);

// Generate text
var result = await pipeline.GenerateTextAsync(
    "Explain the concept of machine learning in simple terms",
    new TextGenerationParameters
    {
        MaxTokens = 500,
        Temperature = 0.7,
        TopP = 0.9,
        TopK = 40,
        RepeatPenalty = 1.1
    });

Console.WriteLine(result.Data);</code></pre>
                    </div>

                    <h3>Code Generation with CodeLlama</h3>
                    <div class="code-example">
                        <h4>Local Code Generation</h4>
                        <pre><code class="language-csharp">// Configure for code generation
var codeConfig = new LocalTransformerConfig
{
    ModelPath = @"C:\Models\CodeLlama-13B-Instruct.gguf",
    ModelFormat = ModelFormat.GGUF,
    Device = ComputeDevice.CUDA,
    ContextLength = 8192  // Larger context for code
};

var codePipeline = TransformerPipelineFactory.CreateLocalPipeline(
    runtimeManager, executeManager, codeConfig);

await codePipeline.InitializeAsync(new TransformerPipelineConfig
{
    TaskType = TransformerTask.CodeGeneration
});

await codePipeline.LoadModelAsync(new TransformerModelInfo
{
    Name = "CodeLlama-13B-Instruct",
    Source = TransformerModelSource.Local
}, TransformerTask.CodeGeneration);

var codePrompt = @"
Write a C# method that implements a binary search algorithm.
The method should:
1. Take a sorted array and a target value
2. Return the index of the target if found, -1 if not found
3. Include proper error handling
4. Be well-documented with XML comments
";

var codeResult = await codePipeline.GenerateTextAsync(
    codePrompt,
    new TextGenerationParameters
    {
        MaxTokens = 1000,
        Temperature = 0.1,  // Low temperature for code
        TopP = 0.95,
        StopSequences = new[] { "\n\n\n", "```" }
    });

Console.WriteLine("Generated Code:");
Console.WriteLine(codeResult.Data);</code></pre>
                    </div>

                    <h3>Multi-Model Management</h3>
                    <div class="code-example">
                        <h4>Managing Multiple Local Models</h4>
                        <pre><code class="language-csharp">public class LocalModelManager
{
    private readonly Dictionary<string, LocalTransformerPipeline> _loadedModels = new();
    private readonly LocalModelConfig _config;

    public async Task<LocalTransformerPipeline> GetModelAsync(string modelName)
    {
        if (_loadedModels.ContainsKey(modelName))
        {
            return _loadedModels[modelName];
        }

        var modelConfig = GetModelConfig(modelName);
        var pipeline = TransformerPipelineFactory.CreateLocalPipeline(
            runtimeManager, executeManager, modelConfig);

        await pipeline.InitializeAsync(new TransformerPipelineConfig
        {
            TaskType = GetTaskTypeForModel(modelName)
        });

        await pipeline.LoadModelAsync(new TransformerModelInfo
        {
            Name = modelName,
            Source = TransformerModelSource.Local,
            LocalPath = modelConfig.ModelPath
        }, GetTaskTypeForModel(modelName));

        _loadedModels[modelName] = pipeline;
        return pipeline;
    }

    public async Task<string> GenerateWithBestModelAsync(string prompt, TransformerTask task)
    {
        var modelName = SelectBestModelForTask(task);
        var pipeline = await GetModelAsync(modelName);
        
        var result = await pipeline.GenerateTextAsync(prompt);
        return result.Success ? result.Data : "Generation failed";
    }

    private LocalTransformerConfig GetModelConfig(string modelName)
    {
        return modelName switch
        {
            "llama-2-7b" => new LocalTransformerConfig
            {
                ModelPath = @"C:\Models\llama-2-7b-chat.gguf",
                Device = ComputeDevice.CUDA,
                ContextLength = 4096,
                BatchSize = 8
            },
            "codellama-13b" => new LocalTransformerConfig
            {
                ModelPath = @"C:\Models\CodeLlama-13B-Instruct.gguf",
                Device = ComputeDevice.CUDA,
                ContextLength = 8192,
                BatchSize = 4  // Larger model, smaller batch
            },
            "mistral-7b" => new LocalTransformerConfig
            {
                ModelPath = @"C:\Models\mistral-7b-instruct.gguf",
                Device = ComputeDevice.CUDA,
                ContextLength = 8192,
                BatchSize = 12
            },
            _ => throw new ArgumentException($"Unknown model: {modelName}")
        };
    }

    private string SelectBestModelForTask(TransformerTask task)
    {
        return task switch
        {
            TransformerTask.CodeGeneration => "codellama-13b",
            TransformerTask.TextGeneration => "mistral-7b",
            TransformerTask.ChatCompletion => "llama-2-7b",
            _ => "llama-2-7b"
        };
    }
}</code></pre>
                    </div>

                    <h3>Performance Optimization</h3>
                    <div class="code-example">
                        <h4>Optimizing Inference Speed</h4>
                        <pre><code class="language-csharp">public class LocalModelOptimizer
{
    public async Task<LocalTransformerConfig> OptimizeConfigAsync(string modelPath)
    {
        var systemInfo = await GetSystemInfoAsync();
        
        return new LocalTransformerConfig
        {
            ModelPath = modelPath,
            ModelFormat = DetectModelFormat(modelPath),
            
            HardwareConfig = new HardwareConfig
            {
                Device = SelectOptimalDevice(systemInfo),
                GpuLayers = CalculateOptimalGpuLayers(systemInfo, modelPath),
                ThreadCount = CalculateOptimalThreads(systemInfo),
                BatchSize = CalculateOptimalBatchSize(systemInfo),
                UseMlock = systemInfo.AvailableRAM > 16_000_000_000, // 16GB+
                UseMmap = true
            },
            
            QuantizationConfig = new QuantizationConfig
            {
                ModelQuantization = SelectOptimalQuantization(systemInfo),
                KvCacheQuantization = QuantizationType.Q8_0
            }
        };
    }

    private ComputeDevice SelectOptimalDevice(SystemInfo info)
    {
        if (info.HasCudaGpu && info.CudaVRAM > 8_000_000_000) // 8GB VRAM
            return ComputeDevice.CUDA;
        if (info.HasMetalGpu && info.MetalRAM > 8_000_000_000) // 8GB Metal
            return ComputeDevice.Metal;
        if (info.AvailableRAM > 32_000_000_000) // 32GB RAM
            return ComputeDevice.CPU;
        
        return ComputeDevice.CPU;
    }

    private int CalculateOptimalGpuLayers(SystemInfo info, string modelPath)
    {
        var modelSize = GetModelSize(modelPath);
        var availableVRAM = info.CudaVRAM * 0.8; // Use 80% of VRAM
        
        // Estimate layers based on model size and available VRAM
        var estimatedLayerSize = modelSize / GetModelLayerCount(modelPath);
        var maxLayers = (int)(availableVRAM / estimatedLayerSize);
        
        return Math.Min(maxLayers, GetModelLayerCount(modelPath));
    }

    private int CalculateOptimalThreads(SystemInfo info)
    {
        // Use 75% of available cores for inference
        return Math.Max(1, (int)(info.CpuCores * 0.75));
    }

    private int CalculateOptimalBatchSize(SystemInfo info)
    {
        if (info.HasCudaGpu && info.CudaVRAM > 16_000_000_000)
            return 16;
        if (info.HasCudaGpu && info.CudaVRAM > 8_000_000_000)
            return 8;
        if (info.AvailableRAM > 32_000_000_000)
            return 8;
        
        return 4;
    }
}</code></pre>
                    </div>
                </section>

                <!-- Model Formats -->
                <section class="section" id="formats">
                    <h2>Supported Model Formats</h2>
                    
                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4><i class="bi bi-file-earmark-binary"></i> GGUF</h4>
                            <p><strong>Recommended Format</strong></p>
                            <ul>
                                <li>Latest format from llama.cpp</li>
                                <li>Excellent compression</li>
                                <li>Fast loading and inference</li>
                                <li>Quantization support</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-file-earmark-code"></i> SafeTensors</h4>
                            <p><strong>HuggingFace Native</strong></p>
                            <ul>
                                <li>Secure tensor format</li>
                                <li>Memory-mapped loading</li>
                                <li>Direct HF compatibility</li>
                                <li>Good for fine-tuned models</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-file-earmark-text"></i> ONNX</h4>
                            <p><strong>Cross-Platform</strong></p>
                            <ul>
                                <li>Optimized for inference</li>
                                <li>Hardware acceleration</li>
                                <li>Smaller model sizes</li>
                                <li>Good CPU performance</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-file-earmark"></i> PyTorch</h4>
                            <p><strong>Research & Development</strong></p>
                            <ul>
                                <li>Full model flexibility</li>
                                <li>Easy customization</li>
                                <li>Development-friendly</li>
                                <li>Larger file sizes</li>
                            </ul>
                        </div>
                    </div>

                    <div class="code-example">
                        <h4>Model Format Detection</h4>
                        <pre><code class="language-csharp">public static ModelFormat DetectModelFormat(string modelPath)
{
    var extension = Path.GetExtension(modelPath).ToLower();
    var fileName = Path.GetFileName(modelPath).ToLower();
    
    return extension switch
    {
        ".gguf" => ModelFormat.GGUF,
        ".ggml" => ModelFormat.GGML,
        ".bin" when fileName.Contains("safetensors") => ModelFormat.SafeTensors,
        ".safetensors" => ModelFormat.SafeTensors,
        ".onnx" => ModelFormat.ONNX,
        ".pt" or ".pth" => ModelFormat.PyTorch,
        _ when Directory.Exists(modelPath) => DetectDirectoryFormat(modelPath),
        _ => ModelFormat.Unknown
    };
}

private static ModelFormat DetectDirectoryFormat(string directoryPath)
{
    var files = Directory.GetFiles(directoryPath);
    
    if (files.Any(f => f.EndsWith(".safetensors")))
        return ModelFormat.SafeTensors;
    if (files.Any(f => f.EndsWith(".gguf")))
        return ModelFormat.GGUF;
    if (files.Any(f => f.EndsWith(".bin")))
        return ModelFormat.PyTorch;
    
    return ModelFormat.Unknown;
}</code></pre>
                    </div>
                </section>

                <!-- Best Practices -->
                <section class="section" id="best-practices">
                    <h2>?? Best Practices</h2>
                    
                    <div class="tip">
                        <strong>? Performance Tips</strong>
                        <ul>
                            <li>Use GGUF format for best performance</li>
                            <li>Enable GPU acceleration when available</li>
                            <li>Use appropriate quantization for your hardware</li>
                            <li>Implement model preloading for production</li>
                        </ul>
                    </div>

                    <div class="success">
                        <strong>?? Security Considerations</strong>
                        <ul>
                            <li>Validate model files before loading</li>
                            <li>Use separate directories for different model sources</li>
                            <li>Implement resource limits and timeouts</li>
                            <li>Monitor system resources during inference</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <strong>?? Resource Management</strong>
                        <ul>
                            <li>Monitor memory usage with large models</li>
                            <li>Implement proper cleanup and disposal</li>
                            <li>Use model rotation for memory efficiency</li>
                            <li>Set appropriate context length limits</li>
                        </ul>
                    </div>
                </section>

                <!-- Related Classes -->
                <section class="section" id="related">
                    <h2>?? Related Classes</h2>
                    <ul class="member-list">
                        <li>
                            <div class="member-name">BaseTransformerPipeline</div>
                            <div class="member-type">Abstract Class</div>
                            <div class="member-desc">Base implementation with common pipeline functionality</div>
                        </li>
                        <li>
                            <div class="member-name">LocalTransformerConfig</div>
                            <div class="member-type">Configuration Class</div>
                            <div class="member-desc">Configuration options for local model execution</div>
                        </li>
                        <li>
                            <div class="member-name">HuggingFaceTransformerPipeline</div>
                            <div class="member-type">Pipeline Class</div>
                            <div class="member-desc">Alternative HuggingFace integration for model downloads</div>
                        </li>
                    </ul>
                </section>

                <!-- Navigation Links -->
                <div class="nav-links">
                    <a href="GoogleTransformerPipeline.html" class="btn-beep">
                        <i class="bi bi-arrow-left"></i> Google AI Pipeline
                    </a>
                    <a href="CustomTransformerPipeline.html" class="btn-beep">
                        <i class="bi bi-arrow-right"></i> Custom Pipeline
                    </a>
                </div>

                <!-- Footer -->
                <footer class="documentation-footer">
                    <div class="footer-content">
                        <div class="footer-copyright">
                            <p>&copy; 2024 The Tech Idea - Beep.Python.AI.Transformers API Documentation</p>
                        </div>
                        <div class="footer-links">
                            <a href="../index.html">Home</a>
                            <a href="ITransformerPipeLine.html">ITransformerPipeLine</a>
                            <a href="TransformerPipelineFactory.html">Pipeline Factory</a>
                        </div>
                    </div>
                </footer>
            </div>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/navigation.js"></script>
</body>
</html>