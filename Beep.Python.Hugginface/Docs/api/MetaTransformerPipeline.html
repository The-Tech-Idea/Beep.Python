<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MetaTransformerPipeline API - Beep.Python.AI.Transformers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <link href="../assets/styles.css" rel="stylesheet">
</head>
<body>
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">
        <i class="bi bi-list"></i>
    </button>

    <!-- Theme Toggle -->
    <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
        <i class="bi bi-sun-fill" id="theme-icon"></i>
    </button>

    <div class="container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <!-- Navigation will be loaded dynamically by navigation.js -->
        </aside>

        <!-- Main Content -->
        <main class="content">
            <div class="content-wrapper">
                <!-- Breadcrumb Navigation -->
                <div class="breadcrumb-nav">
                    <a href="../index.html">Home</a>
                    <span>›</span>
                    <a href="../index.html#core-api">Core API</a>
                    <span>›</span>
                    <span>MetaTransformerPipeline</span>
                </div>

                <!-- Page Header -->
                <div class="page-header">
                    <h1><i class="bi bi-robot"></i> MetaTransformerPipeline Class</h1>
                    <p class="page-subtitle">Advanced integration with Meta AI's Llama family of open-source language models</p>
                </div>

                <!-- Class Info -->
                <div class="class-info">
                    <div class="class-namespace">
                        <strong>Namespace:</strong> Beep.Python.AI.Transformers<br>
                        <strong>Assembly:</strong> Beep.Python.AI.Transformers.dll<br>
                        <strong>Package:</strong> Beep.Python.AI.Transformers
                    </div>
                    <div class="class-implements">
                        <strong>Inheritance:</strong> BaseTransformerPipeline ? MetaTransformerPipeline<br>
                        <strong>Implements:</strong> ITransformerPipeLine, IDisposable
                    </div>
                </div>

                <!-- Overview Section -->
                <section class="section" id="overview">
                    <h2>?? Overview</h2>
                    <p>
                        The <code>MetaTransformerPipeline</code> class provides optimized integration with Meta AI's family of Llama models. 
                        This includes Llama 2, Code Llama, and various fine-tuned variants. The pipeline is specifically optimized for 
                        Meta's model architecture with automatic quantization, efficient memory management, and proper prompt formatting.
                    </p>

                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4><i class="bi bi-cpu"></i> Optimized Performance</h4>
                            <p>Hardware-specific optimizations and quantization support</p>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-code-square"></i> Code Generation</h4>
                            <p>Specialized support for Code Llama models</p>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-shield-check"></i> Open Source</h4>
                            <p>Free to use with full transparency and customization</p>
                        </div>
                        <div class="feature-card">
                            <h4><i class="bi bi-memory"></i> Memory Efficient</h4>
                            <p>8-bit and 4-bit quantization for resource optimization</p>
                        </div>
                    </div>

                    <div class="highlight-box">
                        <h4>?? Supported Meta Models</h4>
                        <ul>
                            <li><strong>Llama 2:</strong> 7B, 13B, and 70B parameter models</li>
                            <li><strong>Llama 2-Chat:</strong> Instruction-tuned conversational models</li>
                            <li><strong>Code Llama:</strong> Specialized code generation models</li>
                            <li><strong>Code Llama-Instruct:</strong> Instruction-following code models</li>
                            <li><strong>Alpaca:</strong> Fine-tuned instruction-following variants</li>
                            <li><strong>Vicuna:</strong> Community fine-tuned conversational models</li>
                        </ul>
                    </div>
                </section>

                <!-- Constructor Section -->
                <section class="section" id="constructor">
                    <h2>?? Constructor</h2>
                    
                    <div class="code-example">
                        <h4>MetaTransformerPipeline(IPythonRunTimeManager, IPythonCodeExecuteManager)</h4>
                        <pre><code class="language-csharp">public MetaTransformerPipeline(
    IPythonRunTimeManager pythonRunTimeManager,
    IPythonCodeExecuteManager executeManager
)</code></pre>
                        <h5>Parameters</h5>
                        <ul>
                            <li><code>pythonRunTimeManager</code> - Python runtime management interface</li>
                            <li><code>executeManager</code> - Python code execution interface</li>
                        </ul>
                    </div>
                </section>

                <!-- Methods Section -->
                <section class="section" id="methods">
                    <h2>?? Methods</h2>

                    <!-- InitializeAsync -->
                    <div class="method">
                        <h3>InitializeAsync</h3>
                        <pre><code class="language-csharp">public override async Task&lt;bool&gt; InitializeAsync(TransformerPipelineConfig config)</code></pre>
                        <p>Initializes the Meta pipeline with optimized packages for Llama models.</p>
                        
                        <h4>Installed Packages</h4>
                        <ul>
                            <li><strong>torch:</strong> PyTorch framework</li>
                            <li><strong>accelerate:</strong> Distributed inference acceleration</li>
                            <li><strong>bitsandbytes:</strong> Quantization support</li>
                            <li><strong>sentencepiece:</strong> Tokenization for Llama models</li>
                        </ul>
                    </div>

                    <!-- LoadModelAsync -->
                    <div class="method">
                        <h3>LoadModelAsync</h3>
                        <pre><code class="language-csharp">public override async Task&lt;bool&gt; LoadModelAsync(
    TransformerModelInfo modelInfo, 
    TransformerTask taskType, 
    Dictionary&lt;string, object&gt;? modelConfig = null
)</code></pre>
                        <p>Loads Meta models from HuggingFace Hub or local directories with automatic optimizations.</p>
                        
                        <h4>Automatic Optimizations</h4>
                        <table>
                            <thead>
                                <tr>
                                    <th>Optimization</th>
                                    <th>Default Value</th>
                                    <th>Purpose</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>torch_dtype</code></td>
                                    <td>torch.float16</td>
                                    <td>Memory efficiency</td>
                                </tr>
                                <tr>
                                    <td><code>device_map</code></td>
                                    <td>auto</td>
                                    <td>Automatic GPU utilization</td>
                                </tr>
                                <tr>
                                    <td><code>load_in_8bit</code></td>
                                    <td>true</td>
                                    <td>8-bit quantization</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <!-- GenerateTextAsync -->
                    <div class="method">
                        <h3>GenerateTextAsync</h3>
                        <pre><code class="language-csharp">public override async Task&lt;TransformerResult&lt;string&gt;&gt; GenerateTextAsync(
    string prompt, 
    TextGenerationParameters? parameters = null
)</code></pre>
                        <p>Generates text with automatic Llama prompt formatting and optimized inference parameters.</p>
                    </div>

                    <!-- GetSupportedTasks -->
                    <div class="method">
                        <h3>GetSupportedTasks</h3>
                        <pre><code class="language-csharp">public override List&lt;TransformerTask&gt; GetSupportedTasks()</code></pre>
                        <p>Returns tasks optimized for Meta models:</p>
                        <ul>
                            <li>TextGeneration</li>
                            <li>Conversational</li>
                            <li>TextClassification</li>
                            <li>Summarization</li>
                            <li>QuestionAnswering</li>
                            <li>FeatureExtraction</li>
                            <li>FillMask</li>
                        </ul>
                    </div>
                </section>

                <!-- Usage Examples -->
                <section class="section" id="examples">
                    <h2>?? Usage Examples</h2>

                    <!-- Basic Llama 2 Example -->
                    <div class="example">
                        <h3>1. Basic Llama 2 Text Generation</h3>
                        <pre><code class="language-csharp">using Beep.Python.AI.Transformers;

// Initialize Meta pipeline
var metaPipeline = new MetaTransformerPipeline(runtimeManager, executeManager);
await metaPipeline.InitializeAsync(new TransformerPipelineConfig());

// Load Llama 2 7B model
var modelInfo = new TransformerModelInfo
{
    Name = "meta-llama/Llama-2-7b-hf",
    Source = TransformerModelSource.HuggingFace
};

// Model config with optimizations
var modelConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["device_map"] = "auto",
    ["load_in_8bit"] = true  // Enable 8-bit quantization
};

await metaPipeline.LoadModelAsync(modelInfo, TransformerTask.TextGeneration, modelConfig);

// Generate text
var result = await metaPipeline.GenerateTextAsync(
    "The future of artificial intelligence is",
    new TextGenerationParameters 
    { 
        MaxLength = 150,
        Temperature = 0.7,
        TopP = 0.9,
        DoSample = true
    });

if (result.Success)
{
    Console.WriteLine($"Llama 2 Output: {result.Data}");
    Console.WriteLine($"Model Family: {result.Metadata?["meta_model_family"]}");
}
</code></pre>
                    </div>

                    <!-- Code Llama Example -->
                    <div class="example">
                        <h3>2. Code Llama for Programming</h3>
                        <pre><code class="language-csharp">// Load Code Llama model
var codeModelInfo = new TransformerModelInfo
{
    Name = "codellama/CodeLlama-7b-Python-hf",
    Source = TransformerModelSource.HuggingFace
};

var codeConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["load_in_4bit"] = true,  // Even more aggressive quantization
    ["bnb_4bit_compute_dtype"] = "torch.float16",
    ["bnb_4bit_quant_type"] = "nf4"
};

await metaPipeline.LoadModelAsync(codeModelInfo, TransformerTask.TextGeneration, codeConfig);

// Generate Python code
var codePrompt = @"# Python function to implement binary search
def binary_search(arr, target):
    """"""
    Implement binary search algorithm to find target in sorted array.
    Returns index if found, -1 if not found.
    """"""";

var codeResult = await metaPipeline.GenerateTextAsync(codePrompt,
    new TextGenerationParameters 
    { 
        MaxLength = 200,
        Temperature = 0.1,  // Lower temperature for more focused code
        TopP = 0.95
    });

if (codeResult.Success)
{
    Console.WriteLine("Generated Code:");
    Console.WriteLine(codeResult.Data);
}

// Code completion example
var completionPrompt = @"import pandas as pd
import numpy as np

# Load data from CSV file
df = pd.read_csv('data.csv')

# Calculate summary statistics";

var completion = await metaPipeline.GenerateTextAsync(completionPrompt,
    new TextGenerationParameters 
    { 
        MaxLength = 100,
        Temperature = 0.2
    });
</code></pre>
                    </div>

                    <!-- Llama 2 Chat Example -->
                    <div class="example">
                        <h3>3. Llama 2 Chat Model</h3>
                        <pre><code class="language-csharp">// Load Llama 2 Chat model
var chatModelInfo = new TransformerModelInfo
{
    Name = "meta-llama/Llama-2-7b-chat-hf",
    Source = TransformerModelSource.HuggingFace
};

await metaPipeline.LoadModelAsync(chatModelInfo, TransformerTask.Conversational, modelConfig);

// The pipeline automatically formats chat prompts
var chatPrompt = "Explain the difference between machine learning and deep learning";

var chatResult = await metaPipeline.GenerateTextAsync(chatPrompt,
    new TextGenerationParameters 
    { 
        MaxLength = 300,
        Temperature = 0.6,
        RepetitionPenalty = 1.1
    });

if (chatResult.Success)
{
    Console.WriteLine("Llama 2 Chat Response:");
    Console.WriteLine(chatResult.Data);
    
    // The formatted prompt is available in metadata
    Console.WriteLine($"Formatted Prompt: {chatResult.Metadata?["meta_formatted_prompt"]}");
}

// Multi-turn conversation
var conversationHistory = new List&lt;string&gt;
{
    "What is Python?",
    "Python is a high-level programming language...",
    "Can you show me a simple Python example?"
};

// Format as conversation (implementation would build proper chat template)
var conversationPrompt = FormatLlamaConversation(conversationHistory);
var response = await metaPipeline.GenerateTextAsync(conversationPrompt);
</code></pre>
                    </div>

                    <!-- Local Model Example -->
                    <div class="example">
                        <h3>4. Local Model Loading</h3>
                        <pre><code class="language-csharp">// Load local Llama model
var localModelInfo = new TransformerModelInfo
{
    Name = "LocalLlama",
    Source = TransformerModelSource.Local,
    ModelPath = @"C:\Models\llama-2-7b-chat"
};

var localConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["device_map"] = "auto",
    ["local_files_only"] = true,  // Don't download from HuggingFace
    ["trust_remote_code"] = true
};

await metaPipeline.LoadModelAsync(localModelInfo, TransformerTask.TextGeneration, localConfig);

// Generate with local model
var localResult = await metaPipeline.GenerateTextAsync(
    "Write a short story about space exploration",
    new TextGenerationParameters 
    { 
        MaxLength = 250,
        Temperature = 0.8,
        TopK = 50
    });

if (localResult.Success)
{
    Console.WriteLine($"Local Model Output: {localResult.Data}");
    Console.WriteLine($"Execution Time: {localResult.ExecutionTimeMs}ms");
}
</code></pre>
                    </div>
                </section>

                <!-- Advanced Configuration -->
                <section class="section" id="advanced-config">
                    <h2>?? Advanced Configuration</h2>

                    <!-- Quantization Settings -->
                    <div class="config-section">
                        <h3>Quantization Configuration</h3>
                        <p>Optimize memory usage with different quantization levels:</p>
                        
                        <div class="code-example">
                            <h4>8-bit Quantization (Default)</h4>
                            <pre><code class="language-json">{
  "load_in_8bit": true,
  "llm_int8_threshold": 6.0,
  "llm_int8_has_fp16_weight": false
}</code></pre>
                        </div>

                        <div class="code-example">
                            <h4>4-bit Quantization (Maximum Efficiency)</h4>
                            <pre><code class="language-json">{
  "load_in_4bit": true,
  "bnb_4bit_compute_dtype": "torch.float16",
  "bnb_4bit_quant_type": "nf4",
  "bnb_4bit_use_double_quant": true
}</code></pre>
                        </div>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Quantization</th>
                                        <th>Memory Usage</th>
                                        <th>Quality</th>
                                        <th>Speed</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>No Quantization</td>
                                        <td>100%</td>
                                        <td>Best</td>
                                        <td>Fast</td>
                                    </tr>
                                    <tr>
                                        <td>8-bit</td>
                                        <td>~50%</td>
                                        <td>Very Good</td>
                                        <td>Good</td>
                                    </tr>
                                    <tr>
                                        <td>4-bit</td>
                                        <td>~25%</td>
                                        <td>Good</td>
                                        <td>Acceptable</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <!-- Device Configuration -->
                    <div class="config-section">
                        <h3>Device and Hardware Configuration</h3>
                        <pre><code class="language-csharp">// GPU configuration
var gpuConfig = new Dictionary&lt;string, object&gt;
{
    ["device_map"] = "auto",  // Automatic GPU distribution
    ["torch_dtype"] = "torch.float16",
    ["max_memory"] = new Dictionary&lt;string, string&gt;
    {
        ["0"] = "10GB",  // GPU 0 memory limit
        ["1"] = "10GB",  // GPU 1 memory limit
        ["cpu"] = "30GB" // CPU memory limit
    }
};

// CPU-only configuration
var cpuConfig = new Dictionary&lt;string, object&gt;
{
    ["device_map"] = "cpu",
    ["torch_dtype"] = "torch.float32",  // Float32 for CPU
    ["low_cpu_mem_usage"] = true
};

// Single GPU configuration
var singleGpuConfig = new Dictionary&lt;string, object&gt;
{
    ["device_map"] = "cuda:0",
    ["torch_dtype"] = "torch.float16"
};</code></pre>
                    </div>

                    <!-- Model-specific Settings -->
                    <div class="config-section">
                        <h3>Model-specific Optimizations</h3>
                        <pre><code class="language-csharp">// Code Llama specific settings
var codeLlamaConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["load_in_4bit"] = true,
    ["max_new_tokens"] = 1024,  // Longer outputs for code
    ["temperature"] = 0.1,      // More deterministic for code
    ["code_mode"] = true        // Custom flag for code-specific optimizations
};

// Llama 2 Chat specific settings
var chatLlamaConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["load_in_8bit"] = true,
    ["chat_template"] = true,    // Enable chat formatting
    ["system_message"] = "You are a helpful assistant.",
    ["max_new_tokens"] = 512
};</code></pre>
                    </div>
                </section>

                <!-- Performance Optimization -->
                <section class="section" id="performance">
                    <h2>?? Performance Optimization</h2>

                    <div class="feature-section">
                        <h3>Memory Management</h3>
                        <p>Tips for optimizing memory usage with large Llama models:</p>
                        
                        <div class="tip">
                            <h4>Memory Optimization Strategies</h4>
                            <ul>
                                <li><strong>Quantization:</strong> Use 8-bit or 4-bit quantization for smaller models</li>
                                <li><strong>Gradient Checkpointing:</strong> Trade compute for memory during training</li>
                                <li><strong>Model Sharding:</strong> Distribute large models across multiple GPUs</li>
                                <li><strong>CPU Offloading:</strong> Move inactive layers to CPU memory</li>
                            </ul>
                        </div>

                        <pre><code class="language-csharp">// Memory-optimized configuration for large models
var memoryOptimizedConfig = new Dictionary&lt;string, object&gt;
{
    ["load_in_8bit"] = true,
    ["device_map"] = "auto",
    ["low_cpu_mem_usage"] = true,
    ["torch_dtype"] = "torch.float16",
    ["offload_folder"] = "./offload",  // CPU offload directory
    ["offload_state_dict"] = true
};</code></pre>
                    </div>

                    <div class="feature-section">
                        <h3>Inference Speed</h3>
                        <p>Optimize inference speed for production workloads:</p>
                        
                        <div class="warning">
                            <h4>Speed Optimization Tips</h4>
                            <ul>
                                <li><strong>Batch Processing:</strong> Process multiple prompts together</li>
                                <li><strong>KV Caching:</strong> Cache key-value pairs for faster generation</li>
                                <li><strong>Prompt Caching:</strong> Cache embeddings for repeated prompts</li>
                                <li><strong>Model Compilation:</strong> Use torch.compile for faster execution</li>
                            </ul>
                        </div>

                        <pre><code class="language-csharp">// Speed-optimized configuration
var speedConfig = new Dictionary&lt;string, object&gt;
{
    ["torch_dtype"] = "torch.float16",
    ["device_map"] = "cuda:0",
    ["use_cache"] = true,           // Enable KV caching
    ["torch_compile"] = true,       // Enable compilation
    ["attn_implementation"] = "flash_attention_2"  // Use Flash Attention
};</code></pre>
                    </div>
                </section>

                <!-- Best Practices -->
                <section class="section" id="best-practices">
                    <h2>? Best Practices</h2>

                    <div class="tip">
                        <h4>??? Model Selection</h4>
                        <ul>
                            <li><strong>7B Models:</strong> Good balance of quality and speed for most tasks</li>
                            <li><strong>13B Models:</strong> Better quality for complex reasoning tasks</li>
                            <li><strong>70B Models:</strong> Highest quality but requires significant resources</li>
                            <li><strong>Code Llama:</strong> Specialized for programming tasks</li>
                            <li><strong>Chat Models:</strong> Optimized for conversational use cases</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h4>? Resource Management</h4>
                        <ul>
                            <li><strong>GPU Memory:</strong> Monitor VRAM usage, especially with large models</li>
                            <li><strong>Model Loading:</strong> Load models once and reuse across requests</li>
                            <li><strong>Batch Size:</strong> Adjust batch size based on available memory</li>
                            <li><strong>Cleanup:</strong> Properly dispose of models when done</li>
                        </ul>
                    </div>

                    <div class="note">
                        <h4>?? Production Deployment</h4>
                        <ul>
                            <li><strong>Model Hosting:</strong> Use dedicated inference servers for production</li>
                            <li><strong>Load Balancing:</strong> Distribute requests across multiple model instances</li>
                            <li><strong>Monitoring:</strong> Track performance metrics and resource usage</li>
                            <li><strong>Versioning:</strong> Maintain model versions for reproducibility</li>
                        </ul>
                    </div>
                </section>

                <!-- Navigation Links -->
                <div class="nav-links">
                    <a href="CohereTransformerPipeline.html" class="btn-beep">
                        <i class="bi bi-arrow-left"></i> CohereTransformerPipeline
                    </a>
                    <a href="MistralTransformerPipeline.html" class="btn-beep">
                        <i class="bi bi-arrow-right"></i> MistralTransformerPipeline
                    </a>
                </div>

                <!-- Footer -->
                <footer class="documentation-footer">
                    <div class="footer-content">
                        <div class="footer-copyright">
                            <p>&copy; 2024 The Tech Idea - Beep.Python.AI.Transformers API Documentation</p>
                        </div>
                        <div class="footer-links">
                            <a href="../index.html">Home</a>
                            <a href="ITransformerPipeLine.html">ITransformerPipeLine</a>
                            <a href="TransformerPipelineFactory.html">Pipeline Factory</a>
                        </div>
                    </div>
                </footer>
            </div>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/navigation.js"></script>
</body>
</html>