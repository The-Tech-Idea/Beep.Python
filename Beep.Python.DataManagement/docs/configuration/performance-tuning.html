<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Performance Tuning Guide - Beep.Python.DataManagement</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <link href="../assets/styles.css" rel="stylesheet">
</head>
<body>
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">
        <i class="bi bi-list"></i>
    </button>

    <!-- Theme Toggle -->
    <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
        <i class="bi bi-sun-fill" id="theme-icon"></i>
    </button>

    <div class="container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <!-- Navigation will be loaded dynamically by navigation.js -->
        </aside>

        <!-- Main Content -->
        <main class="content">
            <div class="content-wrapper">
                <!-- Breadcrumb Navigation -->
                <div class="breadcrumb-nav">
                    <a href="../index.html">Home</a>
                    <span>?</span>
                    <a href="../index.html#configuration">Configuration</a>
                    <span>?</span>
                    <span>Performance Tuning</span>
                </div>

                <!-- Page Header -->
                <div class="page-header">
                    <h1><i class="bi bi-speedometer2"></i> Performance Tuning Guide</h1>
                    <p class="page-subtitle">Comprehensive performance optimization strategies for Beep.Python.DataManagement in production environments</p>
                </div>

                <!-- Overview -->
                <section class="section" id="overview">
                    <h2>?? Performance Optimization Overview</h2>
                    
                    <div class="note">
                        <h4>?? Key Performance Areas</h4>
                        <p>
                            Optimize your pandas operations across multiple dimensions: <strong>memory management</strong>, 
                            <strong>computation speed</strong>, <strong>I/O operations</strong>, and <strong>concurrent processing</strong> 
                            to achieve maximum throughput in enterprise environments.
                        </p>
                    </div>

                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4><i class="bi bi-memory"></i> Memory Optimization</h4>
                            <p>Efficient memory usage, garbage collection, and DataFrame optimization</p>
                        </div>
                        
                        <div class="feature-card">
                            <h4><i class="bi bi-cpu"></i> Computation Speed</h4>
                            <p>Vectorization, parallel processing, and algorithm optimization</p>
                        </div>
                        
                        <div class="feature-card">
                            <h4><i class="bi bi-hdd"></i> I/O Performance</h4>
                            <p>Efficient data loading, chunked processing, and format optimization</p>
                        </div>
                        
                        <div class="feature-card">
                            <h4><i class="bi bi-lightning"></i> Concurrency</h4>
                            <p>Multi-threading, session management, and resource pooling</p>
                        </div>
                    </div>
                </section>

                <!-- Memory Optimization -->
                <section class="section" id="memory-optimization">
                    <h2>?? Memory Optimization Strategies</h2>
                    
                    <div class="code-example">
                        <h4>1. DataFrame Memory Optimization</h4>
                        <pre><code class="language-csharp">public class DataFrameMemoryOptimizer
{
    private readonly IPythonPandasManager _pandasManager;
    private readonly ILogger&lt;DataFrameMemoryOptimizer&gt; _logger;
    
    public DataFrameMemoryOptimizer(IPythonPandasManager pandasManager, ILogger&lt;DataFrameMemoryOptimizer&gt; logger)
    {
        _pandasManager = pandasManager;
        _logger = logger;
    }
    
    public async Task&lt;MemoryOptimizationResult&gt; OptimizeDataFrameMemory(string dataFrameName, MemoryOptimizationConfig config = null)
    {
        var result = new MemoryOptimizationResult { DataFrameName = dataFrameName };
        
        // Get initial memory usage
        var initialMemory = await GetMemoryUsage(dataFrameName);
        result.InitialMemoryMB = initialMemory;
        
        _logger.LogInformation("Starting memory optimization for {DataFrameName}. Initial memory: {Memory}MB", 
            dataFrameName, initialMemory);
        
        // Apply memory optimization techniques
        await OptimizeDataTypes(dataFrameName, result);
        await OptimizeCategoricalColumns(dataFrameName, result);
        await OptimizeStringColumns(dataFrameName, result);
        await RemoveUnusedColumns(dataFrameName, result, config);
        await OptimizeIndexes(dataFrameName, result);
        
        // Get final memory usage
        var finalMemory = await GetMemoryUsage(dataFrameName);
        result.FinalMemoryMB = finalMemory;
        result.MemorySavedMB = initialMemory - finalMemory;
        result.MemorySavedPercentage = (result.MemorySavedMB / initialMemory) * 100;
        
        _logger.LogInformation("Memory optimization completed. Saved: {Saved}MB ({Percentage:F1}%)", 
            result.MemorySavedMB, result.MemorySavedPercentage);
        
        return result;
    }
    
    private async Task OptimizeDataTypes(string dataFrameName, MemoryOptimizationResult result)
    {
        string optimizationCode = $@"
import pandas as pd
import numpy as np

def optimize_numeric_dtypes(df):
    """"""Optimize numeric data types to reduce memory usage""""""
    optimizations = []
    
    for col in df.select_dtypes(include=[np.number]).columns:
        original_dtype = df[col].dtype
        original_memory = df[col].memory_usage(deep=True)
        
        if df[col].dtype == 'int64':
            # Try to downcast integers
            min_val, max_val = df[col].min(), df[col].max()
            
            if min_val >= 0:  # Unsigned integers
                if max_val <= 255:
                    df[col] = df[col].astype('uint8')
                elif max_val <= 65535:
                    df[col] = df[col].astype('uint16')
                elif max_val <= 4294967295:
                    df[col] = df[col].astype('uint32')
            else:  # Signed integers
                if -128 <= min_val and max_val <= 127:
                    df[col] = df[col].astype('int8')
                elif -32768 <= min_val and max_val <= 32767:
                    df[col] = df[col].astype('int16')
                elif -2147483648 <= min_val and max_val <= 2147483647:
                    df[col] = df[col].astype('int32')
        
        elif df[col].dtype == 'float64':
            # Try to downcast floats
            if df[col].min() >= np.finfo(np.float32).min and df[col].max() <= np.finfo(np.float32).max:
                # Check if precision is preserved
                float32_version = df[col].astype('float32')
                if np.allclose(df[col], float32_version, equal_nan=True):
                    df[col] = float32_version
        
        new_memory = df[col].memory_usage(deep=True)
        if new_memory < original_memory:
            optimizations.append({{
                'column': col,
                'original_dtype': str(original_dtype),
                'new_dtype': str(df[col].dtype),
                'memory_saved': original_memory - new_memory
            }})
    
    return df, optimizations

# Apply numeric optimizations
{dataFrameName}_optimized, numeric_optimizations = optimize_numeric_dtypes({dataFrameName})

# Update the original DataFrame
{dataFrameName} = {dataFrameName}_optimized

print(f'Numeric dtype optimizations: {{len(numeric_optimizations)}} columns optimized')
for opt in numeric_optimizations:
    print(f'  {{opt["column"]}}: {{opt["original_dtype"]}} ? {{opt["new_dtype"]}} (saved {{opt["memory_saved"]/1024/1024:.2f}}MB)')
";
        
        await _pandasManager.ExecutePythonCodeAsync(optimizationCode);
        result.OptimizationSteps.Add("Optimized numeric data types");
    }
    
    private async Task OptimizeCategoricalColumns(string dataFrameName, MemoryOptimizationResult result)
    {
        string categoricalCode = $@"
import pandas as pd
import numpy as np

def optimize_categorical_columns(df, categorical_threshold=0.5):
    """"""Convert suitable string columns to categorical type""""""
    optimizations = []
    
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].dtype == 'object':
            # Calculate uniqueness ratio
            unique_ratio = df[col].nunique() / len(df[col].dropna())
            
            # Convert to categorical if uniqueness is below threshold
            if unique_ratio <= categorical_threshold:
                original_memory = df[col].memory_usage(deep=True)
                
                # Convert to category
                df[col] = df[col].astype('category')
                
                new_memory = df[col].memory_usage(deep=True)
                memory_saved = original_memory - new_memory
                
                if memory_saved > 0:
                    optimizations.append({{
                        'column': col,
                        'unique_ratio': unique_ratio,
                        'memory_saved': memory_saved,
                        'categories': df[col].cat.categories.tolist()[:10]  # First 10 categories
                    }})
    
    return df, optimizations

# Apply categorical optimizations
{dataFrameName}_cat_optimized, categorical_optimizations = optimize_categorical_columns({dataFrameName})

# Update the DataFrame
{dataFrameName} = {dataFrameName}_cat_optimized

print(f'Categorical optimizations: {{len(categorical_optimizations)}} columns converted')
for opt in categorical_optimizations:
    print(f'  {{opt["column"]}}: {{opt["unique_ratio"]:.2f}} uniqueness ratio (saved {{opt["memory_saved"]/1024/1024:.2f}}MB)')
";
        
        await _pandasManager.ExecutePythonCodeAsync(categoricalCode);
        result.OptimizationSteps.Add("Converted suitable columns to categorical type");
    }
    
    private async Task OptimizeStringColumns(string dataFrameName, MemoryOptimizationResult result)
    {
        string stringOptCode = $@"
import pandas as pd

def optimize_string_columns(df):
    """"""Optimize string columns using string dtype and deduplication""""""
    optimizations = []
    
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].dtype == 'object':
            original_memory = df[col].memory_usage(deep=True)
            
            # Convert to string dtype (more memory efficient for strings)
            try:
                df[col] = df[col].astype('string')
                new_memory = df[col].memory_usage(deep=True)
                
                if new_memory < original_memory:
                    optimizations.append({{
                        'column': col,
                        'original_dtype': 'object',
                        'new_dtype': 'string',
                        'memory_saved': original_memory - new_memory
                    }})
            except Exception as e:
                # If conversion fails, keep original
                print(f'Could not convert {{col}} to string dtype: {{e}}')
    
    return df, optimizations

# Apply string optimizations  
{dataFrameName}_str_optimized, string_optimizations = optimize_string_columns({dataFrameName})

# Update the DataFrame
{dataFrameName} = {dataFrameName}_str_optimized

print(f'String optimizations: {{len(string_optimizations)}} columns optimized')
for opt in string_optimizations:
    print(f'  {{opt["column"]}}: {{opt["original_dtype"]}} ? {{opt["new_dtype"]}} (saved {{opt["memory_saved"]/1024/1024:.2f}}MB)')
";
        
        await _pandasManager.ExecutePythonCodeAsync(stringOptCode);
        result.OptimizationSteps.Add("Optimized string column storage");
    }
    
    private async Task&lt;decimal&gt; GetMemoryUsage(string dataFrameName)
    {
        string memoryCode = $@"
memory_usage_bytes = {dataFrameName}.memory_usage(deep=True).sum()
memory_usage_mb = memory_usage_bytes / 1024 / 1024
print(f'Memory usage: {{memory_usage_mb:.2f}} MB')
";
        
        await _pandasManager.ExecutePythonCodeAsync(memoryCode);
        return 125.6m; // Would extract from Python output
    }
}</code></pre>
                    </div>

                    <div class="code-example">
                        <h4>2. Automated Memory Management</h4>
                        <pre><code class="language-csharp">public class MemoryManager
{
    private readonly IPythonPandasManager _pandasManager;
    private readonly ILogger&lt;MemoryManager&gt; _logger;
    
    public async Task&lt;MemoryStatus&gt; MonitorMemoryUsage()
    {
        string memoryMonitoringCode = @"
import psutil
import gc
import pandas as pd

# System memory information
system_memory = psutil.virtual_memory()
process = psutil.Process()
process_memory = process.memory_info()

# Python garbage collection
gc.collect()  # Force garbage collection

# Get DataFrame memory usage summary
dataframe_memory = 0
dataframe_count = 0

# List all DataFrames in the current namespace
for name, obj in list(locals().items()):
    if isinstance(obj, pd.DataFrame):
        df_memory = obj.memory_usage(deep=True).sum()
        dataframe_memory += df_memory
        dataframe_count += 1

memory_status = {
    'system': {
        'total_gb': system_memory.total / (1024**3),
        'available_gb': system_memory.available / (1024**3),
        'used_percentage': system_memory.percent
    },
    'process': {
        'rss_mb': process_memory.rss / (1024**2),
        'vms_mb': process_memory.vms / (1024**2)
    },
    'dataframes': {
        'count': dataframe_count,
        'total_memory_mb': dataframe_memory / (1024**2)
    }
}

print(f'System Memory: {memory_status[""system""][""used_percentage""]:.1f}% used')
print(f'Process Memory: {memory_status[""process""][""rss_mb""]:.1f} MB')
print(f'DataFrames: {dataframe_count} total, {memory_status[""dataframes""][""total_memory_mb""]:.1f} MB')
";
        
        await _pandasManager.ExecutePythonCodeAsync(memoryMonitoringCode);
        
        return new MemoryStatus
        {
            SystemMemoryUsedPercentage = 65.2m,
            ProcessMemoryMB = 512.8m,
            DataFrameCount = 8,
            DataFrameMemoryMB = 256.4m
        };
    }
    
    public async Task PerformMemoryCleanup(MemoryCleanupConfig config)
    {
        _logger.LogInformation("Starting memory cleanup operation");
        
        string cleanupCode = $@"
import gc
import pandas as pd
from datetime import datetime, timedelta

cleanup_results = {{
    'dataframes_removed': 0,
    'memory_freed_mb': 0,
    'garbage_collected': 0
}}

# Get current time for age calculation
current_time = datetime.now()

# Clean up old DataFrames (if tracking creation time)
dataframes_to_remove = []

# In a real implementation, you would track DataFrame creation times
# For now, we'll simulate cleanup of large, unused DataFrames

for name, obj in list(locals().items()):
    if isinstance(obj, pd.DataFrame):
        df_memory = obj.memory_usage(deep=True).sum()
        
        # Remove DataFrames larger than threshold if specified
        max_size_mb = {config?.MaxDataFrameSizeMB ?? 100}
        if df_memory > max_size_mb * 1024 * 1024:
            # Check if DataFrame name suggests it's temporary
            if any(keyword in name.lower() for keyword in ['temp', 'tmp', 'intermediate', 'cache']):
                dataframes_to_remove.append((name, df_memory))

# Remove identified DataFrames
for name, memory in dataframes_to_remove:
    del locals()[name]
    cleanup_results['dataframes_removed'] += 1
    cleanup_results['memory_freed_mb'] += memory / (1024**2)

# Force garbage collection
gc_collected = gc.collect()
cleanup_results['garbage_collected'] = gc_collected

print(f'Cleanup completed:')
print(f'  DataFrames removed: {{cleanup_results[""dataframes_removed""]}}')
print(f'  Memory freed: {{cleanup_results[""memory_freed_mb""]:.2f}} MB')
print(f'  Objects garbage collected: {{cleanup_results[""garbage_collected""]}}')
";
        
        await _pandasManager.ExecutePythonCodeAsync(cleanupCode);
        _logger.LogInformation("Memory cleanup completed");
    }
}</code></pre>
                    </div>
                </section>

                <!-- Computation Speed -->
                <section class="section" id="computation-speed">
                    <h2>? Computation Speed Optimization</h2>
                    
                    <div class="code-example">
                        <h4>1. Vectorization and Parallel Processing</h4>
                        <pre><code class="language-csharp">public class ComputationOptimizer
{
    private readonly IPythonPandasManager _pandasManager;
    
    public async Task&lt;ComputationOptimizationResult&gt; OptimizeComputations(
        string dataFrameName, 
        ComputationOptimizationConfig config)
    {
        var result = new ComputationOptimizationResult();
        
        // Configure pandas for optimal performance
        await ConfigurePandasPerformance();
        
        // Enable parallel processing where beneficial
        await EnableParallelProcessing(config);
        
        // Optimize common operations
        await OptimizeAggregations(dataFrameName, result);
        await OptimizeTransformations(dataFrameName, result);
        await OptimizeJoins(dataFrameName, result);
        
        return result;
    }
    
    private async Task ConfigurePandasPerformance()
    {
        string perfConfigCode = @"
import pandas as pd
import numpy as np
import warnings

# Optimize pandas settings for performance
pd.set_option('mode.chained_assignment', None)  # Disable chained assignment warnings
pd.set_option('compute.use_bottleneck', True)   # Use bottleneck for fast ops
pd.set_option('compute.use_numexpr', True)      # Use numexpr for complex expressions

# Configure numpy for better performance
np.seterr(divide='ignore', invalid='ignore')   # Ignore division warnings for speed

# Suppress warnings that can slow down operations
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

print('Pandas performance settings optimized')
";
        
        await _pandasManager.ExecutePythonCodeAsync(perfConfigCode);
    }
    
    private async Task EnableParallelProcessing(ComputationOptimizationConfig config)
    {
        string parallelCode = $@"
import multiprocessing
import pandas as pd
from joblib import Parallel, delayed
import numpy as np

# Configure parallel processing
num_cores = multiprocessing.cpu_count()
max_workers = min(num_cores, {config?.MaxWorkers ?? 4})

print(f'Configuring parallel processing with {{max_workers}} workers')

def parallel_apply(df, func, n_cores=max_workers):
    """"""Apply function to DataFrame chunks in parallel""""""
    if len(df) < 1000:  # Don't parallelize small DataFrames
        return df.apply(func)
    
    # Split DataFrame into chunks
    chunk_size = len(df) // n_cores
    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]
    
    # Process chunks in parallel
    results = Parallel(n_jobs=n_cores)(delayed(lambda chunk: chunk.apply(func))(chunk) for chunk in chunks)
    
    # Combine results
    return pd.concat(results, ignore_index=True)

def parallel_groupby_apply(df, groupby_cols, func, n_cores=max_workers):
    """"""Apply function to groups in parallel""""""
    grouped = df.groupby(groupby_cols)
    
    if len(grouped) < n_cores:  # More cores than groups
        return grouped.apply(func)
    
    # Get group keys and split into chunks
    group_keys = list(grouped.groups.keys())
    key_chunks = np.array_split(group_keys, n_cores)
    
    # Process group chunks in parallel
    def process_group_chunk(keys):
        chunk_groups = []
        for key in keys:
            if isinstance(key, tuple):
                group_data = df[df[groupby_cols].apply(tuple, axis=1) == key]
            else:
                group_data = df[df[groupby_cols[0]] == key]
            chunk_groups.append(func(group_data))
        return chunk_groups
    
    results = Parallel(n_jobs=n_cores)(delayed(process_group_chunk)(chunk) for chunk in key_chunks)
    
    # Flatten and combine results
    flat_results = [item for sublist in results for item in sublist]
    return pd.concat(flat_results, ignore_index=True)

# Store parallel functions in global scope for later use
parallel_processing_enabled = True
";
        
        await _pandasManager.ExecutePythonCodeAsync(parallelCode);
    }
    
    private async Task OptimizeAggregations(string dataFrameName, ComputationOptimizationResult result)
    {
        string aggOptCode = $@"
import pandas as pd
import numpy as np

def optimize_aggregations(df):
    """"""Optimize common aggregation patterns""""""
    optimizations = []
    
    # Use built-in methods instead of apply when possible
    # Example optimizations:
    
    # 1. Use sum() instead of apply(lambda x: x.sum())
    # 2. Use mean() instead of apply(lambda x: x.mean())
    # 3. Use agg() for multiple operations instead of multiple calls
    
    # Demonstrate efficient aggregation patterns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if len(numeric_cols) > 0:
        # Efficient multi-aggregation
        agg_funcs = ['sum', 'mean', 'min', 'max', 'std']
        
        # Instead of multiple separate operations:
        # df[col].sum(), df[col].mean(), etc.
        # Use single agg call:
        multi_agg = df[numeric_cols].agg(agg_funcs)
        
        optimizations.append('Used multi-function aggregation instead of separate calls')
        
        # For groupby operations, use efficient aggregation
        if len(df) > 1000:  # Only for larger datasets
            # Example: efficient groupby aggregation
            sample_group_col = df.columns[0]  # Use first column as example grouper
            
            # Efficient groupby with multiple aggregations
            grouped_agg = df.groupby(sample_group_col)[numeric_cols].agg({{
                col: ['sum', 'mean'] for col in numeric_cols[:3]  # Limit to first 3 for demo
            }})
            
            optimizations.append('Used efficient groupby multi-aggregation')
    
    return optimizations

# Apply aggregation optimizations to the DataFrame
{dataFrameName}_agg_optimizations = optimize_aggregations({dataFrameName})

print('Aggregation optimizations applied:')
for opt in {dataFrameName}_agg_optimizations:
    print(f'  - {{opt}}')
";
        
        await _pandasManager.ExecutePythonCodeAsync(aggOptCode);
        result.OptimizationSteps.Add("Optimized aggregation operations");
    }
}</code></pre>
                    </div>

                    <div class="code-example">
                        <h4>2. Advanced Performance Techniques</h4>
                        <pre><code class="language-csharp">private async Task OptimizeVectorizedOperations(string dataFrameName, ComputationOptimizationResult result)
{
    string vectorizationCode = $@"
import pandas as pd
import numpy as np
import numba
from numba import jit

# Vectorization optimization examples
def optimize_vectorized_operations(df):
    """"""Replace slow operations with vectorized equivalents""""""
    optimizations = []
    
    # 1. Replace iterrows() with vectorized operations
    # Bad: for index, row in df.iterrows(): ...
    # Good: Use vectorized operations
    
    # 2. Use numpy functions for mathematical operations
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols[:3]:  # Demo with first 3 numeric columns
        if col in df.columns:
            # Example: Instead of apply(lambda x: x**2), use vectorized power
            # df[col].apply(lambda x: x**2)  # Slow
            df[f'{{col}}_squared'] = df[col] ** 2  # Fast vectorized
            optimizations.append(f'Vectorized square operation for {{col}}')
            
            # Example: Conditional operations
            # Instead of apply with if-else, use np.where
            df[f'{{col}}_category'] = np.where(
                df[col] > df[col].median(), 
                'high', 
                'low'
            )
            optimizations.append(f'Vectorized conditional categorization for {{col}}')
    
    # 3. Use string vectorized operations for text columns
    text_cols = df.select_dtypes(include=['object', 'string']).columns
    
    for col in text_cols[:2]:  # Demo with first 2 text columns
        if col in df.columns and df[col].dtype == 'object':
            # Vectorized string operations
            try:
                df[f'{{col}}_length'] = df[col].str.len()
                df[f'{{col}}_upper'] = df[col].str.upper()
                optimizations.append(f'Vectorized string operations for {{col}}')
            except Exception as e:
                print(f'Could not apply string operations to {{col}}: {{e}}')
    
    return optimizations

# Numba JIT compilation for custom functions
@jit(nopython=True)
def fast_custom_calculation(arr):
    """"""Example of JIT-compiled function for performance-critical calculations""""""
    result = np.zeros_like(arr)
    for i in range(len(arr)):
        if arr[i] > 0:
            result[i] = np.sqrt(arr[i]) * 2.5
        else:
            result[i] = 0
    return result

# Apply vectorization optimizations
vectorization_optimizations = optimize_vectorized_operations({dataFrameName})

print('Vectorization optimizations applied:')
for opt in vectorization_optimizations:
    print(f'  - {{opt}}')

# Example of using JIT-compiled function
numeric_cols = {dataFrameName}.select_dtypes(include=[np.number]).columns
if len(numeric_cols) > 0:
    first_numeric = numeric_cols[0]
    {dataFrameName}[f'{{first_numeric}}_fast_calc'] = fast_custom_calculation({dataFrameName}[first_numeric].values)
    print(f'Applied JIT-compiled calculation to {{first_numeric}}')
";
    
    await _pandasManager.ExecutePythonCodeAsync(vectorizationCode);
    result.OptimizationSteps.Add("Applied vectorization optimizations");
}</code></pre>
                    </div>
                </section>

                <!-- I/O Performance -->
                <section class="section" id="io-performance">
                    <h2>?? I/O Performance Optimization</h2>
                    
                    <div class="code-example">
                        <h4>1. Efficient Data Loading Strategies</h4>
                        <pre><code class="language-csharp">public class IOPerformanceOptimizer
{
    private readonly IPythonPandasManager _pandasManager;
    
    public async Task&lt;IOOptimizationResult&gt; OptimizeDataLoading(
        string filePath, 
        IOOptimizationConfig config)
    {
        var result = new IOOptimizationResult();
        var extension = Path.GetExtension(filePath).ToLower();
        
        switch (extension)
        {
            case ".csv":
                await OptimizeCSVLoading(filePath, result, config);
                break;
            case ".parquet":
                await OptimizeParquetLoading(filePath, result, config);
                break;
            case ".xlsx":
                await OptimizeExcelLoading(filePath, result, config);
                break;
            default:
                await OptimizeGenericLoading(filePath, result, config);
                break;
        }
        
        return result;
    }
    
    private async Task OptimizeCSVLoading(string filePath, IOOptimizationResult result, IOOptimizationConfig config)
    {
        string csvOptCode = $@"
import pandas as pd
import numpy as np
from pathlib import Path

file_path = r'{filePath}'
optimization_strategies = []

# Get file size for strategy selection
file_size_mb = Path(file_path).stat().st_size / (1024**2)
print(f'File size: {{file_size_mb:.2f}} MB')

if file_size_mb > {config?.LargeFileSizeMB ?? 100}:
    # Large file optimization strategies
    optimization_strategies.append('chunked_reading')
    
    # Strategy 1: Chunked reading with data type optimization
    chunk_size = {config?.ChunkSize ?? 10000}
    chunks = []
    
    # First pass: determine optimal data types
    sample_chunk = pd.read_csv(file_path, nrows=1000)
    
    # Infer optimal dtypes
    dtypes = {{}}
    for col in sample_chunk.columns:
        if sample_chunk[col].dtype == 'object':
            # Check if it should be category
            unique_ratio = sample_chunk[col].nunique() / len(sample_chunk)
            if unique_ratio < 0.5:
                dtypes[col] = 'category'
        elif sample_chunk[col].dtype == 'int64':
            # Try to downcast
            min_val, max_val = sample_chunk[col].min(), sample_chunk[col].max()
            if -128 <= min_val and max_val <= 127:
                dtypes[col] = 'int8'
            elif -32768 <= min_val and max_val <= 32767:
                dtypes[col] = 'int16'
            elif -2147483648 <= min_val and max_val <= 2147483647:
                dtypes[col] = 'int32'
    
    optimization_strategies.append(f'dtype_optimization: {{len(dtypes)}} columns optimized')
    
    # Read file in chunks with optimized dtypes
    for chunk in pd.read_csv(file_path, chunksize=chunk_size, dtype=dtypes):
        chunks.append(chunk)
    
    # Combine chunks
    optimized_df = pd.concat(chunks, ignore_index=True)
    optimization_strategies.append(f'chunked_reading: {{len(chunks)}} chunks processed')
    
else:
    # Small file optimization strategies
    optimization_strategies.append('single_pass_reading')
    
    # Read with engine optimization
    try:
        # Try C engine first (fastest)
        optimized_df = pd.read_csv(file_path, engine='c')
        optimization_strategies.append('used_c_engine')
    except Exception:
        # Fall back to python engine
        optimized_df = pd.read_csv(file_path, engine='python')
        optimization_strategies.append('used_python_engine')

# Additional optimizations
if '{config?.ParseDates ?? false}'.lower() == 'true':
    # Automatic date parsing optimization
    date_columns = []
    for col in optimized_df.columns:
        if 'date' in col.lower() or 'time' in col.lower():
            try:
                pd.to_datetime(optimized_df[col].dropna().head(100))
                date_columns.append(col)
            except:
                pass
    
    if date_columns:
        for col in date_columns:
            optimized_df[col] = pd.to_datetime(optimized_df[col], errors='coerce')
        optimization_strategies.append(f'auto_date_parsing: {{len(date_columns)}} columns')

# Memory optimization post-load
original_memory = optimized_df.memory_usage(deep=True).sum()
optimized_df = optimized_df.convert_dtypes()  # Convert to best possible dtypes
final_memory = optimized_df.memory_usage(deep=True).sum()
memory_saved = (original_memory - final_memory) / (1024**2)

optimization_strategies.append(f'memory_optimization: {{memory_saved:.2f}}MB saved')

# Store optimized DataFrame
optimized_data = optimized_df
csv_optimization_log = optimization_strategies

print('CSV Loading Optimizations Applied:')
for strategy in optimization_strategies:
    print(f'  - {{strategy}}')
";
        
        await _pandasManager.ExecutePythonCodeAsync(csvOptCode);
        result.OptimizationStrategies.AddRange(new[] { "Chunked reading", "Data type optimization", "C engine" });
    }
    
    private async Task OptimizeParquetLoading(string filePath, IOOptimizationResult result, IOOptimizationConfig config)
    {
        string parquetOptCode = $@"
import pandas as pd
import pyarrow.parquet as pq

file_path = r'{filePath}'
optimization_strategies = []

try:
    # Read parquet metadata first for optimization
    parquet_file = pq.ParquetFile(file_path)
    metadata = parquet_file.metadata
    
    print(f'Parquet file info:')
    print(f'  Rows: {{metadata.num_rows:,}}')
    print(f'  Columns: {{len(parquet_file.schema.names)}}')
    print(f'  Row groups: {{metadata.num_row_groups}}')
    
    # Strategy 1: Column selection (if specified)
    columns_to_read = {config?.Columns if config?.Columns?.Any() == true ? $"[{string.Join(", ", config.Columns.Select(c => $"'{c}'"))}]" : "None"}
    
    if columns_to_read:
        # Read only specified columns
        optimized_df = pd.read_parquet(file_path, columns=columns_to_read)
        optimization_strategies.append(f'column_selection: {{len(columns_to_read)}} columns')
    else:
        # Read all columns
        optimized_df = pd.read_parquet(file_path)
        optimization_strategies.append('full_column_read')
    
    # Strategy 2: Row group filtering (if filters specified)
    filters = {config?.Filters if config?.Filters?.Any() == true ? $"[{string.Join(", ", config.Filters)}]" : "None"}
    
    if filters:
        # Apply filters during read (pushdown optimization)
        filtered_df = pd.read_parquet(file_path, filters=filters)
        optimization_strategies.append(f'filter_pushdown: {{len(filters)}} filters')
        optimized_df = filtered_df
    
    # Strategy 3: Memory mapping for large files
    if metadata.num_rows > {config?.LargeRowThreshold ?? 1000000}:
        # Use memory mapping for large files
        optimized_df = pd.read_parquet(file_path, use_nullable_dtypes=True)
        optimization_strategies.append('memory_mapping_large_file')
    
    optimization_strategies.append('parquet_native_read')
    
except Exception as e:
    print(f'Parquet optimization failed: {{e}}')
    # Fall back to standard read
    optimized_df = pd.read_parquet(file_path)
    optimization_strategies.append('fallback_standard_read')

# Store optimized DataFrame
parquet_optimized_data = optimized_df
parquet_optimization_log = optimization_strategies

print('Parquet Loading Optimizations Applied:')
for strategy in optimization_strategies:
    print(f'  - {{strategy}}')
";
        
        await _pandasManager.ExecutePythonCodeAsync(parquetOptCode);
        result.OptimizationStrategies.AddRange(new[] { "Column selection", "Filter pushdown", "Memory mapping" });
    }
}</code></pre>
                    </div>

                    <div class="code-example">
                        <h4>2. Efficient Data Export Strategies</h4>
                        <pre><code class="language-csharp">public async Task&lt;ExportOptimizationResult&gt; OptimizeDataExport(
    string dataFrameName, 
    string outputPath, 
    ExportOptimizationConfig config)
{
    var result = new ExportOptimizationResult();
    var extension = Path.GetExtension(outputPath).ToLower();
    
    string exportOptCode = $@"
import pandas as pd
import numpy as np
from pathlib import Path

df = {dataFrameName}
output_path = r'{outputPath}'
optimization_strategies = []

# Get DataFrame info for optimization decisions
row_count = len(df)
col_count = len(df.columns)
memory_usage = df.memory_usage(deep=True).sum() / (1024**2)

print(f'Exporting DataFrame: {{row_count:,}} rows, {{col_count}} columns, {{memory_usage:.2f}}MB')
";

    switch (extension)
    {
        case ".csv":
            exportOptCode += $@"
# CSV Export Optimizations
if row_count > {config?.LargeRowThreshold ?? 100000}:
    # Large DataFrame optimizations
    optimization_strategies.append('chunked_csv_export')
    
    # Write in chunks to manage memory
    chunk_size = {config?.ChunkSize ?? 10000}
    header = True
    
    for start in range(0, row_count, chunk_size):
        end = min(start + chunk_size, row_count)
        chunk = df.iloc[start:end]
        
        mode = 'w' if start == 0 else 'a'
        chunk.to_csv(output_path, mode=mode, header=header, index=False)
        header = False  # Only write header for first chunk
    
    optimization_strategies.append(f'chunked_export: {{(row_count + chunk_size - 1) // chunk_size}} chunks')
else:
    # Small DataFrame - single write with optimizations
    df.to_csv(output_path, index=False, compression='gzip' if '{config?.UseCompression ?? false}'.lower() == 'true' else None)
    optimization_strategies.append('single_write_csv')
    
    if '{config?.UseCompression ?? false}'.lower() == 'true':
        optimization_strategies.append('gzip_compression')
";
            break;

        case ".parquet":
            exportOptCode += $@"
# Parquet Export Optimizations
compression_type = '{config?.CompressionType ?? "snappy"}'

# Optimize for parquet format
df.to_parquet(
    output_path, 
    engine='pyarrow',
    compression=compression_type,
    index=False
)

optimization_strategies.append(f'parquet_export_{{compression_type}}')
optimization_strategies.append('pyarrow_engine')

# Calculate compression ratio
original_size = memory_usage
compressed_size = Path(output_path).stat().st_size / (1024**2)
compression_ratio = original_size / compressed_size if compressed_size > 0 else 1

optimization_strategies.append(f'compression_ratio: {{compression_ratio:.2f}}x')
";
            break;

        case ".xlsx":
            exportOptCode += $@"
# Excel Export Optimizations
if row_count > 1000000:
    print('Warning: Excel has 1M row limit, data will be truncated')
    df_export = df.head(1000000)
    optimization_strategies.append('row_limit_truncation')
else:
    df_export = df

# Use xlsxwriter for better performance
with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
    df_export.to_excel(writer, sheet_name='Data', index=False)
    
    # Add formatting for better readability
    workbook = writer.book
    worksheet = writer.sheets['Data']
    
    # Auto-adjust column widths
    for i, col in enumerate(df_export.columns):
        max_length = max(len(str(col)), df_export[col].astype(str).str.len().max())
        worksheet.set_column(i, i, min(max_length + 2, 50))  # Cap at 50 chars

optimization_strategies.append('xlsxwriter_engine')
optimization_strategies.append('auto_column_width')
";
            break;
    }

    exportOptCode += $@"

export_optimization_log = optimization_strategies

print('Export Optimizations Applied:')
for strategy in optimization_strategies:
    print(f'  - {{strategy}}')

print(f'File saved to: {{output_path}}')
";

    await _pandasManager.ExecutePythonCodeAsync(exportOptCode);
    
    result.OptimizationStrategies.AddRange(new[] { "Format-specific optimization", "Compression", "Memory management" });
    return result;
}</code></pre>
                    </div>
                </section>

                <!-- Concurrency and Scaling -->
                <section class="section" id="concurrency">
                    <h2>?? Concurrency and Scaling</h2>
                    
                    <div class="code-example">
                        <h4>Session Pool Management</h4>
                        <pre><code class="language-csharp">public class HighPerformanceSessionManager
{
    private readonly ConcurrentDictionary&lt;string, IPythonPandasManager&gt; _sessionPool;
    private readonly SemaphoreSlim _sessionSemaphore;
    private readonly ILogger&lt;HighPerformanceSessionManager&gt; _logger;
    private readonly PerformanceConfig _config;
    
    public HighPerformanceSessionManager(
        ILogger&lt;HighPerformanceSessionManager&gt; logger,
        PerformanceConfig config)
    {
        _sessionPool = new ConcurrentDictionary&lt;string, IPythonPandasManager&gt;();
        _sessionSemaphore = new SemaphoreSlim(config.MaxConcurrentSessions, config.MaxConcurrentSessions);
        _logger = logger;
        _config = config;
    }
    
    public async Task&lt;IPythonPandasManager&gt; GetOptimizedSessionAsync(string userId, string sessionType = "default")
    {
        await _sessionSemaphore.WaitAsync();
        
        try
        {
            var sessionKey = $"{userId}_{sessionType}_{Thread.CurrentThread.ManagedThreadId}";
            
            return _sessionPool.GetOrAdd(sessionKey, key =>
            {
                _logger.LogDebug("Creating new optimized session: {SessionKey}", key);
                
                var manager = CreateOptimizedPandasManager();
                
                // Configure session for optimal performance
                ConfigureSessionForPerformance(manager, sessionType);
                
                return manager;
            });
        }
        catch
        {
            _sessionSemaphore.Release();
            throw;
        }
    }
    
    private IPythonPandasManager CreateOptimizedPandasManager()
    {
        // Implementation would create optimized pandas manager
        // with performance-tuned settings
        return new PythonPandasManager(null, null); // Simplified
    }
    
    private void ConfigureSessionForPerformance(IPythonPandasManager manager, string sessionType)
    {
        // Configure pandas for optimal performance based on session type
        switch (sessionType.ToLower())
        {
            case "analytics":
                // Optimize for analytical workloads
                manager.SetPandasOption("compute.use_bottleneck", "True");
                manager.SetPandasOption("compute.use_numexpr", "True");
                manager.SetPandasOption("mode.chained_assignment", "None");
                break;
                
            case "etl":
                // Optimize for ETL workloads
                manager.SetPandasOption("display.max_columns", "None");
                manager.SetPandasOption("display.max_rows", "None");
                break;
                
            case "reporting":
                // Optimize for reporting workloads
                manager.SetPandasOption("display.precision", "2");
                manager.SetPandasOption("display.float_format", "'{:.2f}'.format");
                break;
        }
    }
    
    public async Task&lt;ConcurrentProcessingResult&gt; ProcessConcurrentOperations&lt;T&gt;(
        IEnumerable&lt;T&gt; items,
        Func&lt;T, IPythonPandasManager, Task&lt;ProcessingResult&gt;&gt; processor,
        ConcurrentProcessingConfig config = null)
    {
        var result = new ConcurrentProcessingResult();
        var semaphore = new SemaphoreSlim(config?.MaxConcurrency ?? Environment.ProcessorCount);
        var tasks = new List&lt;Task&lt;ProcessingResult&gt;&gt;();
        
        _logger.LogInformation("Starting concurrent processing of {ItemCount} items", items.Count());
        
        foreach (var item in items)
        {
            tasks.Add(ProcessItemConcurrently(item, processor, semaphore));
        }
        
        var results = await Task.WhenAll(tasks);
        
        result.TotalItems = items.Count();
        result.SuccessfulItems = results.Count(r => r.Success);
        result.FailedItems = results.Count(r => !r.Success);
        result.ProcessingTime = TimeSpan.FromMilliseconds(results.Max(r => r.ProcessingTimeMs));
        
        _logger.LogInformation("Concurrent processing completed. Success: {Success}, Failed: {Failed}", 
            result.SuccessfulItems, result.FailedItems);
        
        return result;
    }
    
    private async Task&lt;ProcessingResult&gt; ProcessItemConcurrently&lt;T&gt;(
        T item,
        Func&lt;T, IPythonPandasManager, Task&lt;ProcessingResult&gt;&gt; processor,
        SemaphoreSlim semaphore)
    {
        await semaphore.WaitAsync();
        
        try
        {
            var session = await GetOptimizedSessionAsync("system", "concurrent");
            return await processor(item, session);
        }
        finally
        {
            semaphore.Release();
        }
    }
}</code></pre>
                    </div>
                </section>

                <!-- Performance Monitoring -->
                <section class="section" id="monitoring">
                    <h2>?? Performance Monitoring</h2>
                    
                    <div class="code-example">
                        <h4>Real-time Performance Tracking</h4>
                        <pre><code class="language-csharp">public class PerformanceMonitor
{
    private readonly ILogger&lt;PerformanceMonitor&gt; _logger;
    private readonly ConcurrentDictionary&lt;string, PerformanceMetrics&gt; _metrics;
    
    public async Task&lt;PerformanceReport&gt; GeneratePerformanceReport(string dataFrameName)
    {
        string monitoringCode = $@"
import pandas as pd
import psutil
import time
from datetime import datetime

# Collect comprehensive performance metrics
performance_metrics = {{
    'timestamp': datetime.now().isoformat(),
    'dataframe_info': {{}},
    'system_metrics': {{}},
    'operation_benchmarks': {{}}
}}

# DataFrame-specific metrics
df = {dataFrameName}
performance_metrics['dataframe_info'] = {{
    'shape': df.shape,
    'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024**2),
    'dtypes': df.dtypes.value_counts().to_dict(),
    'null_percentage': (df.isnull().sum().sum() / df.size) * 100 if df.size > 0 else 0
}}

# System metrics
system_info = psutil.virtual_memory()
performance_metrics['system_metrics'] = {{
    'memory_total_gb': system_info.total / (1024**3),
    'memory_available_gb': system_info.available / (1024**3),
    'memory_used_percentage': system_info.percent,
    'cpu_count': psutil.cpu_count(),
    'cpu_usage_percentage': psutil.cpu_percent(interval=1)
}}

# Benchmark common operations
if len(df) > 0:
    numeric_cols = df.select_dtypes(include=['number']).columns
    
    if len(numeric_cols) > 0:
        # Benchmark aggregations
        start_time = time.time()
        sum_result = df[numeric_cols].sum()
        aggregation_time = time.time() - start_time
        
        # Benchmark sorting
        start_time = time.time()
        sorted_df = df.sort_values(by=numeric_cols[0])
        sorting_time = time.time() - start_time
        
        # Benchmark filtering
        start_time = time.time()
        median_val = df[numeric_cols[0]].median()
        filtered_df = df[df[numeric_cols[0]] > median_val]
        filtering_time = time.time() - start_time
        
        performance_metrics['operation_benchmarks'] = {{
            'aggregation_time_ms': aggregation_time * 1000,
            'sorting_time_ms': sorting_time * 1000,
            'filtering_time_ms': filtering_time * 1000,
            'rows_per_second_sort': len(df) / sorting_time if sorting_time > 0 else 0,
            'rows_per_second_filter': len(df) / filtering_time if filtering_time > 0 else 0
        }}

print('Performance metrics collected:')
print(f'DataFrame: {{performance_metrics["dataframe_info"]["shape"]}} shape, {{performance_metrics["dataframe_info"]["memory_usage_mb"]:.2f}}MB')
print(f'System: {{performance_metrics["system_metrics"]["memory_used_percentage"]:.1f}}% memory, {{performance_metrics["system_metrics"]["cpu_usage_percentage"]:.1f}}% CPU')
if 'operation_benchmarks' in performance_metrics:
    print(f'Performance: {{performance_metrics["operation_benchmarks"]["rows_per_second_sort"]:,.0f}} rows/sec sort')
";
        
        await _pandasManager.ExecutePythonCodeAsync(monitoringCode);
        
        return new PerformanceReport
        {
            DataFrameName = dataFrameName,
            Timestamp = DateTime.UtcNow,
            MemoryUsageMB = 156.7m,
            CPUUsagePercentage = 23.4m,
            OperationBenchmarks = new Dictionary&lt;string, decimal&gt;
            {
                ["AggregationTimeMs"] = 12.5m,
                ["SortingTimeMs"] = 45.2m,
                ["FilteringTimeMs"] = 8.7m
            }
        };
    }
}</code></pre>
                    </div>
                </section>

                <!-- Best Practices -->
                <section class="section" id="best-practices">
                    <h2>? Performance Best Practices</h2>
                    
                    <div class="tip">
                        <h4>?? Memory Management</h4>
                        <ul>
                            <li><strong>Data Types:</strong> Use appropriate data types (int8 vs int64, category for low-cardinality strings)</li>
                            <li><strong>Cleanup:</strong> Regularly clean up unused DataFrames and force garbage collection</li>
                            <li><strong>Chunking:</strong> Process large datasets in chunks to avoid memory exhaustion</li>
                            <li><strong>Monitoring:</strong> Continuously monitor memory usage and set up alerts</li>
                        </ul>
                    </div>

                    <div class="tip">
                        <h4>? Computation Speed</h4>
                        <ul>
                            <li><strong>Vectorization:</strong> Always prefer vectorized operations over loops or apply()</li>
                            <li><strong>Built-in Methods:</strong> Use pandas built-in methods rather than custom functions</li>
                            <li><strong>Parallel Processing:</strong> Leverage multiprocessing for CPU-intensive operations</li>
                            <li><strong>JIT Compilation:</strong> Use Numba @jit for performance-critical custom functions</li>
                        </ul>
                    </div>

                    <div class="tip">
                        <h4>?? I/O Optimization</h4>
                        <ul>
                            <li><strong>File Formats:</strong> Use Parquet for analytical workloads, HDF5 for numerical data</li>
                            <li><strong>Compression:</strong> Enable compression for network and storage efficiency</li>
                            <li><strong>Column Selection:</strong> Read only required columns when possible</li>
                            <li><strong>Filter Pushdown:</strong> Apply filters during read operations when supported</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h4>?? Common Performance Pitfalls</h4>
                        <ul>
                            <li><strong>Chained Assignment:</strong> Avoid df.loc[mask, col] = value chains</li>
                            <li><strong>Object Dtype:</strong> Convert object columns to appropriate types</li>
                            <li><strong>Memory Leaks:</strong> Always dispose of sessions and clean up DataFrames</li>
                            <li><strong>Unnecessary Copies:</strong> Use inplace=True when appropriate</li>
                        </ul>
                    </div>
                </section>

                <!-- Navigation Links -->
                <div class="nav-links">
                    <a href="virtual-environments.html" class="btn-beep">
                        <i class="bi bi-arrow-left"></i> Virtual Environments
                    </a>
                    <a href="security.html" class="btn-beep">
                        <i class="bi bi-arrow-right"></i> Security & Compliance
                    </a>
                </div>

                <!-- Footer -->
                <footer class="documentation-footer">
                    <div class="footer-content">
                        <div class="footer-copyright">
                            <p>&copy; 2024 The Tech Idea - Beep.Python.DataManagement Documentation</p>
                        </div>
                        <div class="footer-links">
                            <a href="../index.html">Home</a>
                            <a href="../getting-started.html">Getting Started</a>
                            <a href="../api/PythonPandasManager.html">API Reference</a>
                        </div>
                    </div>
                </footer>
            </div>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/navigation.js"></script>
</body>
</html>